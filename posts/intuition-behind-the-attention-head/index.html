<!doctype html><html lang="en" data-mode="light"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="viewport" content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover" ><meta name="generator" content="Jekyll v4.3.3" /><meta property="og:title" content="Intuition Behind the Attention Head of Transformers" /><meta property="og:locale" content="en" /><meta name="description" content="Even as I frequently use transformers for NLP projects, I have struggled with the intuition behind the multi-head attention mechanism outlined in the paper - Attention Is All You Need. This post will act as a memo for my future self." /><meta property="og:description" content="Even as I frequently use transformers for NLP projects, I have struggled with the intuition behind the multi-head attention mechanism outlined in the paper - Attention Is All You Need. This post will act as a memo for my future self." /><link rel="canonical" href="https://yarkhinephyo.github.io/posts/intuition-behind-the-attention-head/" /><meta property="og:url" content="https://yarkhinephyo.github.io/posts/intuition-behind-the-attention-head/" /><meta property="og:site_name" content="Phyo’s Log" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-04-09T06:20:00+00:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Intuition Behind the Attention Head of Transformers" /><meta name="google-site-verification" content="2UleTuNSIWIpwLx2fe8HmZnxBJIFsf8S8208z2wBbc4" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-01-21T02:17:58+00:00","datePublished":"2022-04-09T06:20:00+00:00","description":"Even as I frequently use transformers for NLP projects, I have struggled with the intuition behind the multi-head attention mechanism outlined in the paper - Attention Is All You Need. This post will act as a memo for my future self.","headline":"Intuition Behind the Attention Head of Transformers","mainEntityOfPage":{"@type":"WebPage","@id":"https://yarkhinephyo.github.io/posts/intuition-behind-the-attention-head/"},"url":"https://yarkhinephyo.github.io/posts/intuition-behind-the-attention-head/"}</script><title>Intuition Behind the Attention Head of Transformers | Phyo's Log</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Phyo's Log"><meta name="application-name" content="Phyo's Log"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.25.0/dist/tocbot.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"><body><aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end"><header class="profile-wrapper"> <a href="/" id="avatar" class="rounded-circle"><img src="/assets/img/me_2.jpg" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a><h1 class="site-title"> <a href="/">Phyo's Log</a></h1><p class="site-subtitle fst-italic mb-0">Thoughts on tech and random things that I read. For my future self with goldfish memory.</p></header><nav class="flex-column flex-grow-1 w-100 ps-0"><ul class="nav"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/timeline/" class="nav-link"> <i class="fa-fw fas fa-archive"></i> <span>TIMELINE</span> </a><li class="nav-item"> <a href="/about-me/" class="nav-link"> <i class="fa-fw fas fa-info-circle"></i> <span>ABOUT ME</span> </a></ul></nav><div class="sidebar-bottom d-flex flex-wrap align-items-center w-100"> <a href="https://github.com/yarkhinephyo" aria-label="github" target="_blank" rel="noopener noreferrer" > <i class="fab fa-github"></i> </a> <a href="javascript:location.href = 'mailto:' + ['yarkhinephyo','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="https://www.linkedin.com/in/yar-khine-phyo/" aria-label="linkedin" target="_blank" rel="noopener noreferrer" > <i class="fab fa-linkedin"></i> </a></div></aside><div id="main-wrapper" class="d-flex justify-content-center"><div class="container d-flex flex-column px-xxl-5"><header id="topbar-wrapper" aria-label="Top Bar"><div id="topbar" class="d-flex align-items-center justify-content-between px-lg-3 h-100" ><nav id="breadcrumb" aria-label="Breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Intuition Behind the Attention Head of Transformers</span></nav><button type="button" id="sidebar-trigger" class="btn btn-link"> <i class="fas fa-bars fa-fw"></i> </button><div id="topbar-title"> Post</div><button type="button" id="search-trigger" class="btn btn-link"> <i class="fas fa-search fa-fw"></i> </button> <search class="align-items-center ms-3 ms-lg-0"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..." > </search> <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button></div></header><div class="row flex-grow-1"><main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4"><article class="px-1"><header><h1 data-toc-skip>Intuition Behind the Attention Head of Transformers</h1><div class="post-meta text-muted"> <span> Posted <time data-ts="1649485200" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom" > Apr 9, 2022 </time> </span> <span> Updated <time data-ts="1705803478" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom" > Jan 21, 2024 </time> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/yarkhinephyo">Yar Khine Phyo</a> </em> </span> <span class="readtime" data-bs-toggle="tooltip" data-bs-placement="bottom" title="743 words" > <em>4 min</em> read</span></div></div></header><div class="content"><p>Even as I frequently use transformers for NLP projects, I have struggled with the intuition behind the multi-head attention mechanism outlined in the paper - <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>. This post will act as a memo for my future self.</p><h3 id="limitation-of-only-using-word-embeddings"><span class="me-2">Limitation of only using word embeddings</span><a href="#limitation-of-only-using-word-embeddings" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Consider the sequence of words - <em>pool beats badminton</em>. For the purpose of machine learning tasks, we can use <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> to represent each of them. The representation can be a matrix of three word embeddings.</p><p>If we take a closer look, the word <em>pool</em> has multiple meanings. It can mean a swimming pool, some cue sports or a collection of things such as money. Humans can easily perceive the correct interpretation because of the word <em>badminton</em>. However, the word embedding of <em>pool</em> includes all the possible interpretations learnt from the training corpus.</p><p>Can we add more context to the embedding representing <em>pool</em>? Optimally, we want it to be “aware” of the word <em>badminton</em> more than the word <em>beats</em>.</p><h3 id="my-intuition-behind-the-self-attention-mechanism"><span class="me-2">My intuition behind the self-attention mechanism</span><a href="#my-intuition-behind-the-self-attention-mechanism" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Consider that matrix <strong>A</strong> represents the sequence - <em>pool beats badminton</em>. There are three words (rows) and the word embedding has four dimensions (columns). The first dimension represents the concept of sports. Naturally, we expect the words <em>pool</em> and <em>badminton</em> to have more similarity in this dimension.</p><p><a href="/assets/img/2022-04-09-1.jpg" class="popup img-link shimmer"><img src="/assets/img/2022-04-09-1.jpg" alt="Matrix-A" loading="lazy"></a> <em>Diagram by Author</em></p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span>
  <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>
  <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
  <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
<span class="p">])</span>
</pre></table></code></div></div><p>If we do a matrix multiplication between <strong>A</strong> and <strong>A<sup>T</sup></strong>, the resulting matrix will be the dot-product similarities between all possible pairs of words. For example, the word <em>pool</em> is more similar to <em>badminton</em> than the word <em>beats</em>. In other words, this matrix hints that the word <em>badminton</em> should be more important than the word <em>beats</em> when adding more context to the word embedding of <em>pool</em>.</p><p><a href="/assets/img/2022-04-09-2.jpg" class="popup img-link shimmer"><img src="/assets/img/2022-04-09-2.jpg" alt="Similarity" loading="lazy"></a> <em>Diagram by Author</em></p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre>A_At = np.matmul(A, A.T)
&gt;&gt;&gt; A_At
array([[0.31, 0.14, 0.3 ],
       [0.14, 0.31, 0.15],
       [0.3 , 0.15, 0.31]])
</pre></table></code></div></div><p>By applying the softmax function across each word, we can ensure that these “similarity scores” add up to 1.0.</p><p>The last step is to do another matrix multiplication with matrix <strong>A</strong>. In a way, this step consolidates the contexts of the entire sequence to each embedding in an “intelligent” manner. In the example below, both embeddings of <em>beats</em> and <em>badminton</em> are added to <em>pool</em> but with different weights depending on their similarities with <em>pool</em>.</p><p><a href="/assets/img/2022-04-09-3.jpg" class="popup img-link shimmer"><img src="/assets/img/2022-04-09-3.jpg" alt="Result" loading="lazy"></a> <em>Diagram by Author</em></p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre>output = np.round(
    np.matmul(softmax(A_At, axis=1), A)
, 2)
&gt;&gt;&gt; output
array([[0.38, 0.22, 0.16, 0.14],
       [0.35, 0.25, 0.17, 0.13],
       [0.38, 0.22, 0.17, 0.13]])
</pre></table></code></div></div><p>Notice that the output matrix has the same dimensions (3 x 4) as the original input <strong>A</strong>. The intuition is that each word vector is now enriched with more information. This is the gist of the <ins>self-attention</ins> mechanism.</p><h3 id="scaled-dot-product-attention"><span class="me-2">Scaled Dot-Product Attention</span><a href="#scaled-dot-product-attention" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>The picture below shows the Scaled Dot-Product Attention from the <a href="https://arxiv.org/abs/1706.03762">paper</a>. The core operations are the same as the example we explored. Notice that scaling is added before softmax to ensure stable gradients, and there is an optional masking operation. Inputs are also termed as <strong>Q</strong>, <strong>K</strong> and <strong>V</strong>.</p><p><a href="/assets/img/2022-04-09-4.jpg" class="popup img-link shimmer"><img src="/assets/img/2022-04-09-4.jpg" alt="Result" loading="lazy"></a> <em>Image taken from Attention Is All You Need paper</em></p><p>The Scaled Dot-Product Attention can be represented as <code class="language-plaintext highlighter-rouge">attention(Q, K, V)</code> function.</p><p><a href="/assets/img/2022-04-09-5.jpg" class="popup img-link shimmer"><img src="/assets/img/2022-04-09-5.jpg" alt="Scaled-dot-product" loading="lazy"></a> <em>Diagram by Frank Odom on Medium</em></p><h3 id="adding-trainable-weights-with-linear-layers"><span class="me-2">Adding trainable weights with linear layers</span><a href="#adding-trainable-weights-with-linear-layers" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>The initial example that we use can be represented as <code class="language-plaintext highlighter-rouge">attention(A, A, A)</code>, where matrix <strong>A</strong> contains the word embeddings of <em>pool</em>, <em>beats</em> and <em>badminton</em>. So far there are no weights involved. We can make a simple adjustment to add trainable parameters.</p><p>Imagine we have (m x m) matrices <strong>M<sup>Q</sup></strong>, <strong>M<sup>K</sup></strong> and <strong>M<sup>V</sup></strong> where <em>m</em> matches the dimension of word embeddings in <strong>A</strong>. Instead of passing matrix <strong>A</strong> directly to the function, we can calculate <strong>Q</strong> = <strong>A</strong> <strong>M<sup>Q</sup></strong>, <strong>K</strong> = <strong>A</strong> <strong>M<sup>K</sup></strong> and <strong>V</strong> = <strong>A</strong> <strong>M<sup>V</sup></strong> which will be the same sizes as <strong>A</strong>. Then we apply <code class="language-plaintext highlighter-rouge">attention(Q, K, V)</code> afterwards. In neural network, this is akin to adding a linear layer before each input into the Scaled Dot-Product Attention.</p><p>To complete the Single-Head Attention mechanism, we just need to add another linear layer after the output from the Scaled Dot-Product Attention. The idea of expanding to the Multi-Head Attention in the paper is relatively simpler to grasp.</p><p><a href="/assets/img/2022-04-09-6.jpg" class="popup img-link shimmer"><img src="/assets/img/2022-04-09-6.jpg" alt="Single-head" loading="lazy"></a> <em>Diagram by Frank Odom on Medium</em></p><h3 id="resources"><span class="me-2">Resources</span><a href="#resources" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ol><li><a href="https://www.youtube.com/watch?v=23XUv0T9L5c">Series on Attention by Rasa Algorithm Whiteboard</a><li><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer by Jay Alammer</a></ol></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw me-1"></i> <a href="/categories/tech/">Tech</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw me-1"></i> <a href="/tags/data-science/" class="post-tag no-text-decoration" >Data-Science</a></div><div class=" post-tail-bottom d-flex justify-content-between align-items-center mt-5 pb-2 " ><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper d-flex align-items-center"> <span class="share-label text-muted">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Intuition%20Behind%20the%20Attention%20Head%20of%20Transformers%20-%20Phyo's%20Log&url=https%3A%2F%2Fyarkhinephyo.github.io%2Fposts%2Fintuition-behind-the-attention-head%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Intuition%20Behind%20the%20Attention%20Head%20of%20Transformers%20-%20Phyo's%20Log&u=https%3A%2F%2Fyarkhinephyo.github.io%2Fposts%2Fintuition-behind-the-attention-head%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fyarkhinephyo.github.io%2Fposts%2Fintuition-behind-the-attention-head%2F&text=Intuition%20Behind%20the%20Attention%20Head%20of%20Transformers%20-%20Phyo's%20Log" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <button id="copy-link" aria-label="Copy link" class="btn small" data-bs-toggle="tooltip" data-bs-placement="top" title="Copy link" data-title-succeed="Link copied successfully!" > <i class="fa-fw fas fa-link pe-none fs-6"></i> </button> </span></div></div></div></article></main><aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 mb-5 text-muted"><div class="access"><section id="access-lastmod"><h2 class="panel-heading">Recently Updated</h2><ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2"><li class="text-truncate lh-lg"> <a href="/posts/containers-under-the-hood/">Learning Points: Containers Under the Hood</a><li class="text-truncate lh-lg"> <a href="/posts/hashicorp-vault/">Learning Points: Hashicorp Vault</a><li class="text-truncate lh-lg"> <a href="/posts/cassandra-as-a-service/">Learning Points: Cassandra as a Service (AstraDB)</a><li class="text-truncate lh-lg"> <a href="/posts/large-pages-in-linux-kernel/">Learning Points: Large Pages in the Linux Kernel</a><li class="text-truncate lh-lg"> <a href="/posts/linux-perf-monitoring-tools/">Learning Points: Linux Performance Tools</a></ul></section><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/learning-points/">Learning-Points</a> <a class="post-tag btn btn-outline-primary" href="/tags/system-design/">System-Design</a> <a class="post-tag btn btn-outline-primary" href="/tags/networking/">Networking</a> <a class="post-tag btn btn-outline-primary" href="/tags/operating-system/">Operating-System</a> <a class="post-tag btn btn-outline-primary" href="/tags/database/">Database</a> <a class="post-tag btn btn-outline-primary" href="/tags/c/">C</a> <a class="post-tag btn btn-outline-primary" href="/tags/concurrency/">Concurrency</a> <a class="post-tag btn btn-outline-primary" href="/tags/data-engineering/">Data-Engineering</a> <a class="post-tag btn btn-outline-primary" href="/tags/data-science/">Data-Science</a> <a class="post-tag btn btn-outline-primary" href="/tags/git/">Git</a></div></section></div><section id="toc-wrapper" class="ps-0 pe-4"><h2 class="panel-heading ps-3 pt-2 mb-2">Contents</h2><nav id="toc"></nav></section></aside></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4"><aside id="related-posts" aria-labelledby="related-label"><h3 class="mb-4" id="related-label">Further Reading</h3><nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4"><article class="col"> <a href="/posts/primer-to-sift/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1649813400" data-df="ll" > Apr 13, 2022 </time><h4 class="pt-0 my-2">Primer to Scale-invariant Feature Transform</h4><div class="text-muted"><p> Scale-invariant Feature Transform, also known as SIFT, is a method to consistently represent features in an image even under different scales, rotations and lighting conditions. Since the video ser...</p></div></div></a></article><article class="col"> <a href="/posts/snowflake-new-streams-talk/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1705803600" data-df="ll" > Jan 21, 2024 </time><h4 class="pt-0 my-2">Learning Points: Snowflake Iceberg, Streaming, Unistore</h4><div class="text-muted"><p> This video is about Snowflake Iceberg Tables, Streaming Ingest and Unistore. The presenters are N.Single, T.Jones and A.Motivala as part of the Database Seminar Series by CMU Database Group. Probl...</p></div></div></a></article><article class="col"> <a href="/posts/streamlining-fedramp-compliance/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1705809300" data-df="ll" > Jan 21, 2024 </time><h4 class="pt-0 my-2">Learning Points: Streamlining FedRAMP Compliance with CNCF</h4><div class="text-muted"><p> This video is about streamlining FedRAMP compliance with CNCF technologies. The presenters are Ali Monfre and Vlad Ungureanu from Palantir Technologies. FedRAMP Overview FedRAMP is the accreditat...</p></div></div></a></article></nav></aside><nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation"> <a href="/posts/javascript-module-ecosystem/" class="btn btn-outline-primary" aria-label="Older" ><p>Quick History of JavaScript Module Ecosystem</p></a> <a href="/posts/primer-to-sift/" class="btn btn-outline-primary" aria-label="Newer" ><p>Primer to Scale-invariant Feature Transform</p></a></nav><footer aria-label="Site Info" class=" d-flex flex-column justify-content-center text-muted flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3 " ><p> © <time>2024</time> <a href="https://github.com/yarkhinephyo">Yar Khine Phyo</a>. <span data-bs-toggle="tooltip" data-bs-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author." >Some rights reserved.</span></p><p>Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></footer></div></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-11 content"><div id="search-hints"><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/learning-points/">Learning-Points</a> <a class="post-tag btn btn-outline-primary" href="/tags/system-design/">System-Design</a> <a class="post-tag btn btn-outline-primary" href="/tags/networking/">Networking</a> <a class="post-tag btn btn-outline-primary" href="/tags/operating-system/">Operating-System</a> <a class="post-tag btn btn-outline-primary" href="/tags/database/">Database</a> <a class="post-tag btn btn-outline-primary" href="/tags/c/">C</a> <a class="post-tag btn btn-outline-primary" href="/tags/concurrency/">Concurrency</a> <a class="post-tag btn btn-outline-primary" href="/tags/data-engineering/">Data-Engineering</a> <a class="post-tag btn btn-outline-primary" href="/tags/data-science/">Data-Science</a> <a class="post-tag btn btn-outline-primary" href="/tags/git/">Git</a></div></section></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><aside aria-label="Scroll to Top"> <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow"> <i class="fas fa-angle-up"></i> </button></aside></div><div id="mask"></div><script src="https://cdn.jsdelivr.net/combine/npm/jquery@3.7.1/dist/jquery.min.js,npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js,npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/magnific-popup@1.1.0/dist/jquery.magnific-popup.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.10/dayjs.min.js,npm/dayjs@1.11.10/locale/en.min.js,npm/dayjs@1.11.10/plugin/relativeTime.min.js,npm/dayjs@1.11.10/plugin/localizedFormat.min.js,npm/tocbot@4.25.0/dist/tocbot.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script defer src="/unregister.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-KGM14E7CDB"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-KGM14E7CDB'); }); </script> <script> /* Note: dependent library will be loaded in `js-selector.html` */ SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<article class="px-1 px-sm-2 px-lg-4 px-xl-0"><header><h2><a href="{url}">{title}</a></h2><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div></header><p>{snippet}</p></article>', noResultsText: '<p class="mt-5"></p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
