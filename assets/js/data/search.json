[ { "title": "Learning Points: Containers Under the Hood", "url": "/posts/containers-under-the-hood/", "categories": "Tech", "tags": "Operating-System, Learning-Points", "date": "2024-02-02 07:00:00 +0000", "snippet": "In this video, Gerlof Langeveld from AT Computing talks about how Linux containers work under the hood. The focus is mostly on the Linux namespaces and not Cgroups.Conventional Unix ApproachAll pro...", "content": "In this video, Gerlof Langeveld from AT Computing talks about how Linux containers work under the hood. The focus is mostly on the Linux namespaces and not Cgroups.Conventional Unix ApproachAll processes run in one ecosystem, which includes: Hostname. PID numbers. Mounted filesystems. Network stack. IPC objects (semaphores, pipes). Users.Even before containers, every process could have its own root directory (chroot) which it is limited to for usage.If a process assumes the root identity means all the privileged actions are allowed. Non-root identity meant no privileged actions at all.There were no tools to control resource consumption for each process.Containerized ApproachProcesses are isolated from other processes in the host. Containers are implemented by administering the root directory, namespaces and control groups of a child process differently from the parent. Private filesystem - chroot. Isolated hostname - namespace ‘uts’. Isolated IPC (shmem, semaphores, message queues) - namespace ‘ipc’. Processes in the same namespace can share the objects. Isolated PID numbering - namespace ‘pid’. The process will have different PID in ancestor namespace and its own namespace. Process with PID 1 in any namespace reaps the orphaned children in the namespace. The /proc has to be mounted accordingly in the new PID namespace too. Isolated users - namespace ‘user’. Isolated mount - namespace ‘mnt’. Processes connected to the same namespace share mountpoints. When a new namespace is created, the mount structure is inherited. However, when there are updates to the mount points, there is no impact on the original namespaces. Private network stack - namespace ‘net’. A new network namespace initially has only the loopback interface but more can be added. Physical devices can only be in one namespace. Network namespaces can be connected via veth pairs. Limited privilege under root identity - capabilities. Limited utilization of CPU - cgroup ‘cpu’. Limited utilization of memory - cgroup ‘memory’. Limited utilization of disk - cgroup ‘blkio’.NamespacesEvery process refers to namespaces. When there is a fork, the child processes inherit the binding of namespaces by default. Processes can unshare namespaces.The namespace details are in the pseudo-filesystem /proc. The $$ refers to the current process. The number in the bracket refers to the inode representing the namespace.$ ls -l /proc/$$/nslrwxrwxrwx 1 kyar users 0 Feb 2 02:15 ipc -&gt; 'ipc:[4026531839]'lrwxrwxrwx 1 kyar users 0 Feb 2 02:15 mnt -&gt; 'mnt:[4026531841]'lrwxrwxrwx 1 kyar users 0 Feb 2 02:15 net -&gt; 'net:[4026531840]'lrwxrwxrwx 1 kyar users 0 Feb 2 02:15 pid -&gt; 'pid:[4026531836]'...The unshare command executes a specified program in new namespaces. It uses the system call unshare. The nsenter command connects with existing namespaces of other processes. It uses the system call setns.Docker NamespacesDocker allows sharing of namespaces. For example, docker run --pid=host shares the PID namespace with the host and docker run --pid=container:CID shares the PID namespace with anothe container.Modified Root DirectoryEven before containers were prevalent, every process has own root directory. Usually, all the processes inherit the root directory of the entire filesystem from systemd. Use chroot to use a prepared directory as root. Use pivot_root to change root directory for all processes in the mount namespace.sudo chroot topdir bash --loginCapabilitiesTraditional Unix privilege scheme only checks whether UID = 0. Linux privilege scheme has a collection of distinct privileges that can be set for each process. For example: CAP_CHOWN, CAP_KILL, CAP_SYS_BOOT etc. Thread running with effective UID = 0 initially has all the capabilities set.docker run --cap-add foo --cap-drop bar ...Build a Container Step by Step (Without Cgroups)In the step1.sh, a new hostname namespace is created.#!/bin/bashunshare -u bash step2.shIn step2.sh, the hostname for the child process is changed. Then, unshare is run again to create a new PID namespace. The current process’ environment is not modified. The forked child process will be in the new namespace with PID 1.hostname mycontainerunshare -p --fork --mount-proc bash step3.shIn step3.sh, a new network namespace is created.unshare -n bash step4.shIn step4.sh, the local loopback link is set to up. Then, the script uses nsenter to set up veth pairing interface mybr0 in the parent process’s network stack namespace. Then the script sets up mybr1 device for its own namespace. Now packets transmitted on one device in the pair are immediately received on the other device.At the end, a new mount namespace is created.ip link set dev lo upnsenter -n -t 1 ip link add name mybr0 type veth peer name mybr1 netns $$nsenter -n -t 1 ip addr add 192.168.47.11/24 dev mybr0nsenter -n -t 1 ip link set dev mybr0 upip addr add 192.168.47.12/24 dev my br1ip link set dev mybr1 upunshare -m bash step5.shIn step5.sh, a new directory for the container’s root is created. A tmpfs filesystem (in-memory filesystem) of 50MB is mounted to the new root’s directory. This mount point will be not visible within the parent’s process. The contents from a skeleton root folder is copied into the mount point. pivot_root is used to change the root directory.ROOTDIR=${pwd}/newroot[ -d ${ROOTDIR} ] || mkdir \"$ROOTDIR\"mount -n -t tmpfs -i size=50M none \"${ROOTDIR}\"rsync -a skeletonfs/ \"${ROOTDIR}\"cd \"${ROOTDIR}\"pivot_root . oldrootmount -t proc proc /procexport PS1=\"[\\u$\\h \\W]# \"bash" }, { "title": "Learning Points: Hashicorp Vault", "url": "/posts/hashicorp-vault/", "categories": "Tech", "tags": "System-Design, Learning-Points", "date": "2024-01-30 04:20:00 +0000", "snippet": "In this video, Rob Barnes, a developer advocate from Hashicorp, talks about the services provided by Hashicorp Vault.OverviewVault handles machine-to-machine authentication and authorization. Secr...", "content": "In this video, Rob Barnes, a developer advocate from Hashicorp, talks about the services provided by Hashicorp Vault.OverviewVault handles machine-to-machine authentication and authorization. Secret management platform - Platform to store credentials. Identity broker - Brokers identity on behalf of other platforms and databases. Encryption as a service - Cryptographic solutions to manage own keys or government’s keys.Vault’s fundamental abstraction is the idea of secrets engines. Secrets engines are different Vault components which store, generate or encrypt secrets. The client has to specify with filesystem-like identifiers to select the secret engine to use.Engine Type 1: KV SecretsThe client provides a secret to Vault API. Vault encrypts the secret and stores in a storage backend. If the storage is breached, the secrets remain safe. The applications have to retrieve the secrets through Vault. Vault can also rotate secrets by using a new key for new secrets and re-encrypting the old secrets in storage.In the second version of KV engine, a number of secret versions are retained. this enables the older versions to be retrievable in case of unwanted deletion or updates.Engine Type 2: Dynamic SecretsWith dynamic secrets, Vault generates credentials for a particular database or platform only when the client wants to access it. There are no long-living secrets to be stolen outside of Vault. Imagine that a sample application, there exists a profile API that requires access to a Postgres database. The application authenticates to Vault via methods such as such as OIDC/JWT, Okta, Github etc. Once authenticated, Vault creates new secrets on the target platform with a short-lived TTL. For example, a new user and password are created on the Postgres database. Vault returns a Vault-token which includes authorization details such as which Vault APIs are usable. The application requests for the database credentials with the token. When the TTL is up, Vault revokes the database credentials.Engine Type 3: Encryption ServiceThe burden of encryption and decryption with specific cryptographic protocols is moved from the application developers to Vault. The application sends base64 encoded data to the encryption service by Vault. Vault encrypts the data, appends the encryption versioning and returns the encrypted data. The application stores the encrypted data in Vault or another database. Note that Vault does not act as a proxy in this case.Vault as a ServiceHashicorp cloud platform provides this service in a virtual network via peer connection with the application’s virtual network. For example with peering in Azure, the network traffic stays within the Microsoft backbone infrastructure.Credential LifetimeThe TTL depends on the configuration by the administrator. With longer TTLs, one pattern is to have the application cache the credentials from Vault to avoid refetching every API call." }, { "title": "Learning Points: Cassandra as a Service (AstraDB)", "url": "/posts/cassandra-as-a-service/", "categories": "Tech", "tags": "Database, System-Design, Learning-Points", "date": "2024-01-29 06:00:00 +0000", "snippet": "This video talks about how Apache Cassandra is restructured for the cloud native environment. The presenter is Jake Luciani, the chief architect from DataStax.Cassandra Operational Challenges in Pr...", "content": "This video talks about how Apache Cassandra is restructured for the cloud native environment. The presenter is Jake Luciani, the chief architect from DataStax.Cassandra Operational Challenges in Production All nodes perform all actions in Cassandra. CPU, memory, disk has to scale at the same amount with the original architecture. No isolation between multiple tenants. Cassandra clusters run at maximum scale because there is no in-built elasticity.By 2019, there were Cassandra operators for Kubernetes to make deployments on clouds possible. This works but it does not have the features that other SaaS cloud solutions have.Requirements for an SaaS Cassandra include: Pay for what you use. Can scale storage, compute and network up and down independently. Integrated with the cloud ecosystem. Data can be secured by the users. Simple from the operational standpoint for the customers.Towards Serverless Cassandra Write data into object storage instead of the attached disks. Flushing data from the memtable writes to S3. Use Etcd instead of a custom Java service for the cluster metadata such as schema management and topology changes. Separate functionalities of Cassandra such as authentication, data serving, data compaction into different services. For example, data compaction is moved to AWS lambda while data serving remains in containers. Kubernetes Operators for scaling and upgrades. requests | vcoordination service coordination service &lt;--- | \\ / | Metadata Service v \\ / v (Etcd) data service data service &lt;--- (with cache) (with cache) | | v v Object Storage (S3, GCS, ABS) ^ ^ | | compaction service commitlog replay service (lambda) (lambda)Benefits include: Consistent schema and topology. Independently scalable components. All pods have access to data without streaming. A new node does not have to stream from other nodes when joining the cluster. Local disk is used for predictive caching. No need attached disks in Kubernetes. State management is troublesome with PVCs. Coordinator and data services are stateless. The backing up of data (replication) is done in S3.The new architecture allows multi-tenancy. Each tenant is assigned to a single shared-tenancy Kubernetes cluster. Each tenant has a separate bucket in the object storage service for isolation. A given set of nodes can be part of a ring of one or more databases. Each node may process data from multiple tenants but there are isolations in place. This saves compute for Datastax." }, { "title": "Learning Points: Linux Performance Tools", "url": "/posts/linux-perf-monitoring-tools/", "categories": "Tech", "tags": "Operating-System, Learning-Points", "date": "2024-01-26 08:00:00 +0000", "snippet": "This video talks about some monitoring performance tools in Linux. The presenter is Brendan Gregg from Netflix.Basic Observability Toolstop measures the system and per-process interval summary. Sin...", "content": "This video talks about some monitoring performance tools in Linux. The presenter is Brendan Gregg from Netflix.Basic Observability Toolstop measures the system and per-process interval summary. Since the screen refreshes in intervals, top can miss short lived processes that starts and dies quickly. top can also consume noticeable CPU to read /proc.Load averages measure the resource demand for CPUs + disks in Linux. Time constants of 1, 5 and 15 minutes are used. If there is an increasing order, it means the resources are becoming busier.For CPU, the usage percentages are shown for user-processes, system-processes, nice-configured-processes, idle-time, wait-time-IO, hardware-interrupt-time, software-interrupt-time, wait-time-cpu. The idle-time, wait-time-IO and wait-time-CPU can help to identify overworked systems.For the physical memory, buff/cache means the sum of buffers to be written and in-cache memory for files being read. Therefore, this memory can be used for other purposes after flushing and evicting respectively.top - 01:39:09 up 21:32, 6 users, load average: 0.25, 0.24, 0.21Tasks: 572 total, 1 running, 571 sleeping, 0 stopped, 0 zombie%Cpu(s): 0.2 us, 0.2 sy, 0.0 ni, 99.5 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stMiB Mem : 24061.0 total, 11254.0 free, 1744.2 used, 11062.8 buff/cacheMiB Swap: 8192.0 total, 8192.0 free, 0.0 used. 21854.5 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 168444 13992 8352 S 2.3 0.1 25:10.61 systemd 1048 message+ 20 0 13904 9408 4240 S 0.7 0.0 5:22.55 dbus-daemon 366923 kyar 20 0 13932 4596 3468 R 0.7 0.0 0:00.14 top 15 root 20 0 0 0 0 I 0.3 0.0 0:38.36 rcu_sched 557 root 19 -1 218160 121180 117880 S 0.3 0.5 3:54.25 systemd-journal 954 root 20 0 0 0 0 S 0.3 0.0 0:51.03 kcs-evdefer/1 ps -ef f can shows processes with trees for relationships.UID PID PPID C STIME TTY STAT TIME CMDroot 2 0 0 Jan25 ? S 0:00 [kthreadd]root 3 2 0 Jan25 ? I&lt; 0:00 \\_ [rcu_gp]root 4 2 0 Jan25 ? I&lt; 0:00 \\_ [rcu_par_gp]root 5 2 0 Jan25 ? I&lt; 0:00 \\_ [slub_flushwq] ...vmstat --unit MiB shows virtual memory statistics. Unlike top, we can see the separate memory for buffered writes and memory for cached reads. Under IO, there are the numbers of blocks received and sent (per second) from devices. Under system, there are the numbers of interrupts and context switches per second.procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 0 11278 485 10637 0 0 1 17 14 13 0 1 99 0 0iostat shows the throughputs for each device. Overworked disks can be identified.Device tps kB_read/s kB_wrtn/s kB_dscd/s kB_read kB_wrtn kB_dscdloop0 0.00 0.02 0.00 0.00 1782 0 0loop1 0.01 0.02 0.00 0.00 1868 0 0 ...sda 36.94 20.61 263.88 0.00 1669285 21370268 0sr0 0.00 0.00 0.00 0.00 1 0 0mpstat shows multi-processor statistics to look for hot CPUs.02:16:23 AM CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle02:16:23 AM all 0.21 0.18 0.54 0.21 0.00 0.00 0.00 0.00 0.00 98.8602:16:23 AM 0 0.18 0.26 0.50 0.23 0.00 0.01 0.00 0.00 0.00 98.8202:16:23 AM 1 0.27 0.22 0.68 0.09 0.00 0.01 0.00 0.00 0.00 98.7402:16:23 AM 2 0.20 0.20 0.51 0.27 0.00 0.00 0.00 0.00 0.00 98.81 ...System Call Tracer (strace)Translate system call arguments for better observability. However, it has a massive overhead and can slow the target by &gt; 100 times. Use the -p flag to attach to a process.Network Protocol Statistics (netstat)netstat shows various network protocol statistics.Side note: Unlike internet sockets, unix domain sockets are already reliable and in-order. Unix stream sockets allow reading arbitrary number of bytes. Unix datagrams maintain message boundaries.Active Internet connections (w/o servers)Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 nurseshark.ics.cs.c:ssh c-73-154-235-112.:54352 ESTABLISHEDtcp 0 0 nurseshark.ics.cs.c:ssh c-73-154-235-112.:54101 ESTABLISHEDtcp 0 169 nurseshark.ics.cs:60726 LDAP-06.ANDREW.CM:ldaps ESTABLISHED ...udp 0 0 localhost:41669 localhost:41669 ESTABLISHEDActive UNIX domain sockets (w/o servers)Proto RefCnt Flags Type State I-Node Pathunix 2 [ ] DGRAM 57742 /var/spool/postfix/dev/logunix 2 [ ] DGRAM 35609 @fcm_clifunix 2 [ ] DGRAM 19606476 /run/user/2690276/systemd/notifyunix 16 [ ] DGRAM CONNECTED 21547 /run/systemd/journal/socketunix 3 [ ] STREAM CONNECTED 19592750 /run/user/2707326/busunix 3 [ ] STREAM CONNECTED 18231599 /run/systemd/journal/stdout ...netstat -rn shows the routing table. Gateway 0.0.0.0 means that there is no intermediate hop.Kernel IP routing tableDestination Gateway Genmask Flags MSS Window irtt Iface0.0.0.0 128.2.220.1 0.0.0.0 UG 0 0 0 eno1128.2.1.10 128.2.220.1 255.255.255.255 UGH 0 0 0 eno1128.2.1.11 128.2.220.1 255.255.255.255 UGH 0 0 0 eno1128.2.1.20 128.2.220.1 255.255.255.255 UGH 0 0 0 eno1128.2.1.21 128.2.220.1 255.255.255.255 UGH 0 0 0 eno1 ...128.2.220.0 0.0.0.0 255.255.254.0 U 0 0 0 eno1128.2.220.1 0.0.0.0 255.255.255.255 UH 0 0 0 eno1192.168.122.0 0.0.0.0 255.255.255.0 U 0 0 0 virbr0ip route get gets a single route to a destination and prints its contents exactly as the kernel sees it. In the example, the packet travels from the current machine 128.2.220.25 to the gateway. This aligns with the routing table shown above.8.8.8.8 via 128.2.220.1 dev eno1 src 128.2.220.25 uid 2690276 Note that the loopback address is not inside the routing table. It is because there is a higher priority local table that is consulted at ip route show table local. So packets addressed to 127.0.0.1 or 128.2.220.25 will go to the local machine.local 127.0.0.0/8 dev lo proto kernel scope host src 127.0.0.1 local 127.0.0.1 dev lo proto kernel scope host src 127.0.0.1 broadcast 127.255.255.255 dev lo proto kernel scope link src 127.0.0.1 local 128.2.220.25 dev eno1 proto kernel scope host src 128.2.220.25 broadcast 128.2.221.255 dev eno1 proto kernel scope link src 128.2.220.25 ..." }, { "title": "Learning Points: Large Pages in the Linux Kernel", "url": "/posts/large-pages-in-linux-kernel/", "categories": "Tech", "tags": "Operating-System, Learning-Points, Database", "date": "2024-01-25 12:00:00 +0000", "snippet": "This video talks about Large Pages in the Linux Kernel. The presenter is Matthew Wilcox from Oracle. Related to the topic, this article by Neil Brown talks about challenges of Transparent Huge Page...", "content": "This video talks about Large Pages in the Linux Kernel. The presenter is Matthew Wilcox from Oracle. Related to the topic, this article by Neil Brown talks about challenges of Transparent Huge Pages in the page cache.BackgroundMemory is managed in pages. An Oracle server with 6TiB of memory has 1.5 billion 4KiB pages. As a NUMA system, it has several nodes with a high performance connection. If a processor accesses memory not in its own node, the data is accessed slower than local memory. Per NUMA-node basis, there are 192 million pages which are tracked with an LRU list. A long LRU list is inefficient since there is heavy contention for the lock. TLB cache lines also have high misses. Larger page sizes means less entries in TLBs thus less evictions. Large page table sizes per process, leading to OOM errors. Having many pages means that there is more overhead from the address translation.Huge Pages (HugeTLB)Huge pages are blocks of memory that come in 2MB and 1GB sizes. Huge pages are reserved by the administrators during boot time. They require significant code changes by application developers to be used effectively. For example, mmap can be used with MAP_HUGETBL flag to allocate the huge pages that have been reserved.Compound PageA lower level construct for the kernel developers. Linux can allocate pages in 2n where n is the order of the page. First page is the head page and all the other pages are tail pages. The operation on tail pages usually redirect to the head page. This construct is used to build other systems such as Transparent Huge Pages.Transparent Huge Pages (THP)THP allocates huge pages while being transparent to the applications. The old THP implementation only works for 2 MiB pages and mapping of anonymous memory. Modern kernels support the new THP which works with variable powers of two (4 KiB, 8 KiB, …) in page sizes and added support for tmpfs (shared memory). Unlike the standard Huge Pages, THP allocates page sizes dynamically during runtime.Only some architectures support THP. Sometimes hardware supports larger page sizes, but there is no code in the Linux core. Furthermore, the filesystem authors (besides tmpfs) are unfamiliar with it.Folio APIThere is also an ambiguity on the original THP API. For example, should the filesystem page-fault handle return the head page of THP or a specific subpage?Folio API removes the ambiguity. Any Folio function will operate on the full page. The functions only take in base pages or the head of compound pages, and no tail pages.Controling Large Pages Sizes Hints from userspace have been unreliable in the past. In terms of responsibilities, the filesystem authors should not have to develop their own heuristics. The page cache readahead already decides how many pages to read ahead, so now it should also decide how large pages should be set to. Page fault will allocate PMD-sized (2 MiB) pages if MADV_HUGEPAGE is set by user.Challenges of THP Support in FilesystemsAnonymous memory is only accessed by memory mapping (mmap) and the size of this mapping is usually fixed on allocation. Sharing between processes only happens as the result of a fork and, for the process-private mappings that support THP, the huge pages will only remain shared as long as they are unchanged. So every mapping of an anonymous transparent huge page will be the same size.However, in filesystems, memory used for file-backed data can be mapped concurrently by multiple processes. Preventing one process from getting a huge mapping because the other process only mapped small pages is not acceptable. So THP for filesystems must support concurrent mappings for both huge and non-huge pages.Furthermore, the space in files is not allocated by mapping but by the result of write calls. THP for files must allocate huge pages before the file is known to be big enough to utilize them.Huge Pages in DatabasesStandard huge pages (HugeTLB) can be beneficial as page table sizes are reduced, leading to less memory usage by the kernel.However for THP, the dynamic page sizes means that memory allocation by the kernel may be inefficient for the use case of databases. Database systems rely on their own memory management systems, designed to optimize performance based on the application context. THP can conflict with these built-in memory management strategies." }, { "title": "Learning Points: Getting Started with eBPF", "url": "/posts/getting-started-with-ebpf/", "categories": "Tech", "tags": "C, Networking, Operating-System, Learning-Points", "date": "2024-01-25 03:00:00 +0000", "snippet": "This video is an introductory video on eBPF presented by Liz Rice from Isovalent. In the video description, there are also really interesting hands-on exercises.OverviewAdding features to the kerne...", "content": "This video is an introductory video on eBPF presented by Liz Rice from Isovalent. In the video description, there are also really interesting hands-on exercises.OverviewAdding features to the kernel is slow because of the complexity and the nature of open source community. eBPF allows running custom code in the kernel.Any kernel function call, perf event, network packet can be attached as an event to trigger eBPF programs in the kernel.eBPF Hello WorldThis Python script compiles an eBPF program and attaches it to a kprobe that is hit whenever the execve syscall is run. Then the script prints traces produced by the eBPF program.#!/usr/bin/python3 from bcc import BPFprogram = r\"\"\"int hello(void *ctx) { bpf_trace_printk(\"Hello World!\"); return 0;}\"\"\"b = BPF(text=program)syscall = b.get_syscall_fnname(\"execve\")b.attach_kprobe(event=syscall, fn_name=\"hello\")b.trace_print()Depending on the type of event the eBPF program is attached to, ctx pointer contains different information.eBPF MapeBPF map is a generic data structure that stores key-value data to share between eBPF kernel programs and user-space applications.The macro BPF_HASH creates a hash table that can be updated in the kernel. The BCC framework provides an access to the map in the user-space via b[\"counter_table\"].BPF_HASH(counter_table);int hello(void *ctx) { u64 uid; u64 counter = 0; u64 *p; uid = bpf_get_current_uid_gid() &amp; 0xFFFFFFFF; p = counter_table.lookup(&amp;uid); if (p != 0) { counter = *p; } counter++; counter_table.update(&amp;uid, &amp;counter); return 0;}eBPF RuntimeeBPF program is compiled into bytecode for the eBPF software virtual machine. If we use clang for compilation, the target should be -march=bpf. The BPF syscall verifies the program and loads it into the kernel. After loading into the kernel, the program needs to be attached to the event. More syscalls are used to get the information of eBPF maps into user-space.Usage of bpftoolInstead of using BCC framework, bpftool also provides the control for eBPF programs. Functionalities include: List all the eBPF programs loaded into the kernel. Attach events to eBPF programs. Inspect the bytecodes of eBPF programs. Read the eBPF maps. Update the contents in eBPF naps.XDP with eBPFeXpress Data Path allows running eBPF programs on the network interface card/ driver. However, only some NICs/ drivers support XDP.In the program below, the SEC macro defines the attachment point. As an XDP program, it will be attached to a network interface and triggered whenever an inbound packet is received on that interface. This program counts the number of packets and passes the packets (instead of dropping) to the network stack afterwards.#include &lt;linux/bpf.h&gt;#include &lt;bpf/bpf_helpers.h&gt;int counter = 0;SEC(\"xdp\")int hello(struct xdp_md *ctx) { bpf_printk(\"Hello World %d\", counter); counter++; return XDP_PASS;}char LICENSE[] SEC(\"license\") = \"Dual BSD/GPL\";After compiling with clang, this command loads the eBPF program into the kernel and pin it to the filesystem.bpftool prog load hello.bpf.o /sys/fs/bpf/helloThis command attaches the eBPF program to the loopback network interface.bpftool net attach xdp name hello dev loNote that aside from XDP, the interception of network packets with eBPF can be done at higher levels of the network stack." }, { "title": "Learning Points: Diving into Kafka Internals", "url": "/posts/diving-into-kafka-internals/", "categories": "Tech", "tags": "System-Design, Learning-Points", "date": "2024-01-24 10:50:00 +0000", "snippet": "This video is a deep dive on Apache Kafka’s internals with David Jacot from Confluent. Related to the topic, this article by Confluent talks about why Zookeeper was replaced with KRaft in Kafka.Hig...", "content": "This video is a deep dive on Apache Kafka’s internals with David Jacot from Confluent. Related to the topic, this article by Confluent talks about why Zookeeper was replaced with KRaft in Kafka.High Level Overview of KafkaKafka contains a few components. Kafka cluster: Pub-sub messaging system. Users publish and consume via APIs. Events are persisted and replicated in brokers. Low level APIs: Producer and consumer APIs to interact with Kafka clusters. Connect API: Connectors are built out of the shelf to integrate third party systems with Kafka.Old Metadata on ZookeeperThere are two architectures. There is an ongoing effort to migrate from Zookeeper to KRaft.In the old architecture, Zookeeper elects one controller node as the metadata store and the rest of the broker pool stores data for persistence. The producer and consumer APIs interact with the broker pool.+-----------------------------------+| Zookeeper | Zookeeper | Zookeeper |+-----------------------------------+ | v Controller / | \\ Broker Broker BrokerThere are limitations with this. Broker shutdown: If a broker shuts down, the topic partitions that it acts as a leader will have to be reassigned to a new leader. The controller has to update the metadata in Zookeeper and also propagates to all the brokers. This operation is proportional to the number of brokers. Controller failover: If the old controller crashes, the other brokers will try to register with Zookeeper. The new controller has to fetch metadata from Zookeeper which is proportional to the number of topic partitions in the cluster. During this bootstrap, the controller cannot handle any administrative requests.New Metadata on KRaft Voter ---- Leader ---- Voter (Also brokers) [Metadata Log] / | \\ Broker Broker BrokerInstead of storing metadata in Zookeeper, the metadata is stored as transaction logs in Kafka. The log is managed by a leader and voters in a quorum. The other brokers passively read the log to catch up. Since the replication is done on changelogs, there is no need to worry about divergence by the other brokers.The normal data log replications use primary-backup replication where the leader only considers something committed after all the followers acknowledge a write. The transaction log uses quorum replication where the leader considers a commit after the majority of replicas acknowledges a write. This trades away availability guarantees for better replication latencies.When there is a broker failover, the metadata log is appended within the leader-voter quorum before being replicated by the other brokers. There is no more bottleneck from having a single controller updating all the other brokers. When there is a controller failover, one of the voters will takeover and it will already have the replicated data.KRaft follows the Raft algorithm to achieve quorum replication while piggybacking on Kafkfa’s existing log utilities such as throttling and compression.Topics vs PartitionsTopic is a set of partitions grouped together within the business context. Partitions are distributed in a cluster and every broker is responsible for a subset of them as a leader. The followers replicate the data for durability.Write Path Producer uses an API to write to a given topic. If a key exists, it is hashed to determine the partition. If not, a hash is created. The network layer writes a Produce Request sends to the leader of the partition. The leader does validations and appends the message to its local log in the disk. The leader waits until all the followers are done replicating before sending back the Produce Response.The leaders store a watermark which is the minimum offsets replicated by all the followers. This number is used to decide when to send the Produce Response to the producer. The followers pull from the leader and update their own offsets asynchronously.There is configuration to control the replication consistency (strong vs eventual). It can be set such that the leader responds back to the producer without any followers acknowledging.Duplicate data can be written if replication is done but the Produce Response does not reach the producer (which retries). This is why using idempotency operations is important.Every broker knows the metadata about the cluster such as where the partitions are. The producer refreshes this periodically and assumes it is correct instead of re-checking every Produce Request. On timeout or error, the producer will refresh again.Fetch PathKafka uses a pull model where consumers pull data from the brokers. This means there is no need for brokers to track where the consumers are, which reduces complexity. Consumers also track how far the data has been fetched.Consumer Group Protocol makes sure that in each group, consumers can process more than one partition but partitions can be access by only one consumer. This allows for parallel processing of data in those topics, without reading the same data twice. Every consumer sends Join Group Request to the Group Coordinator (a broker). The Group Coordinator elects a leader from one of the consumers. The group leader assigns partitions to the consumers. Other consumers starts from the latest offsets (the offstes can be committed into the brokers via API).The Group Coordinator manages the group membership and initiates the rebalance activity if consumers stops heartbeats. The Group Leader is only responsible for assignment of partitions but this process happens frequently so the offload of works means less CPU usage for Kafka brokers.Optimization: Number of Partitions Key-based partition: The same key goes into a single partition. From the consumer side, there will be data with the same key in the original order. However, this means that re-partitioning cannot be done. Then it will make sense to over-partition at the beginning. Random hash partition: The messages are placed randomly. There will not be a problem to increase the number of partitions in the future.Currently, there is no way to dynamically change the number of partitions while preserving the ordering.Make sure there are more partitions than the number of consumers." }, { "title": "Learning Points: Weaviate Vector Database", "url": "/posts/weaviate-vector-database/", "categories": "Tech", "tags": "Database, System-Design, Learning-Points", "date": "2024-01-24 04:20:00 +0000", "snippet": "This video is a deep dive on the vector database Weaviate as part of the weekly database seminars by CMU Database Group. The presenter is Etienne Dilocker from Weaviate.Why a Vector DatabaseInstead...", "content": "This video is a deep dive on the vector database Weaviate as part of the weekly database seminars by CMU Database Group. The presenter is Etienne Dilocker from Weaviate.Why a Vector DatabaseInstead of indexing literal keywords from paragraphs, meanings (embeddings) can be indexed for search purposes.LLMs tend to hallucinate less when some context around the question is given.Retrieval Augmented Generation (RAG) retrieves the top few relevant documents from a vector database before providing as a context to LLMs. There is no need to retrain LLMs to keep updated with the latest information.Weaviate ArchitectureCollections are logical groups by the user. Shards distribute data across multiple nodes.In each shard, the HNSW index is used most of the time. The object store can keep any binary files that are related to the embeddings. There is no need to have secondary storage for non-key-value data. The inverted index allows searching by properties and BM25 (bag-of-words retrieval) queries. +-----------------------+ | Weaviate Setup | +-----------------------+ +-- | Collection \"Articles\" | | | Collection \"Authors\" | | | ... | | +-----------------------+ | | +-----------------------+ +-&gt; | Collection \"Articles\" | +-----------------------+ +-- | Shard A | | | Shard B | | | ... | | +-----------------------+ | | +-----------------------+ +-&gt; | Shard A | +-----------------------+ | HNSW Index | | Object Store (LSM) | | Inverted Index (LSM) | +-----------------------+Consistent hashing on a specific key is used for sharding. On each node (physical shard), there can be multiple logical shards.If the number of shards are changed on the fly, there are measures to ensure that minimal amount of data is moved around the nodes.Hierarchical Navigable Small World (HNSW)The index approximates nearest neighbor proximity graph with multiple layers. Compared to other indexes, it is slower to build but faster to query with.Algorithm for querying a Navigable Small World (NSW) graph: Pick a random entry point. Follow all the edges and evaluate if the newly discovered points are closer. Recenter on the best known point. Don’t score any points twice. Stop when there is no more improvement.Considerations when building a NSW graph: A real life dataset with natural clusters would be more efficient as compared to randomly generated points. The number of connections between each point. During the build phase, there is a search on the graph to decide where to place nodes. The more time spent on the build phase, the more efficient the search phase is.Layers of NSW is HNSW. There are fewer connections per point on higher layers of HNSW. Few connections also means the connections “travel” longer distances. The search starts from the higher layers then move to lower layers.Rebuilding NSWAdding new data points do not degrade the graph. When one point has too many connections, pruning is done by reducing first-grade connections (direct) to second-grade connections (indirect).Deleting points degrades the query time. When a point is marked as tombstone, it can still be used for traversing the graph but not be included in the result set. When the proportion of tombstones is large, the graph is rebuilt. On the fly, there are also reassignments of the tombstone’s connections to other points to make sure clusters remain connected. This operation is expensive but works well if there are not too many deletes." }, { "title": "Learning Points: Streamlining FedRAMP Compliance with CNCF", "url": "/posts/streamlining-fedramp-compliance/", "categories": "Tech", "tags": "System-Design, Learning-Points", "date": "2024-01-21 03:55:00 +0000", "snippet": "This video is about streamlining FedRAMP compliance with CNCF technologies. The presenters are Ali Monfre and Vlad Ungureanu from Palantir Technologies.FedRAMP OverviewFedRAMP is the accreditation ...", "content": "This video is about streamlining FedRAMP compliance with CNCF technologies. The presenters are Ali Monfre and Vlad Ungureanu from Palantir Technologies.FedRAMP OverviewFedRAMP is the accreditation required for companies to sell SaaS solutions to the government instead of on-prem solutions. General steps include: Identify a sponsor to join the authorization board. Document security controls and validate them with third-party assessments. Final review with the sponsor and FedRAMP management office.Challenges Pre-Kubernetes Scanning requirements included vulnerability scans, virus scans, configuration scans. They had to be done weekly or monthly basis. Before Kubernetes adoption, there was no uniform infrastructure or standardized immutable AMIs and container images. Palantir had to scan every single piece of live infrastructure. Security patching requirements required constant host reboots. Orchestrating reboot cycles without downtime was a lot of engineering effort. FIPS encryption is the government approved standards for cypher suites and cryptographic libraries. It takes a while for newly introduced cypher suites to be FIPS validated. As Palantir relies on open source technology, maintaining FIPS validated traffic between diverse services was a very difficult problem.How Palantir SolvedFor operating systems, major vendors have STIGs published. Palantir started running immutable machine images which were scanned during the CI process. This provided a faster feedback loop for the developers. Every host was also terminated every 72 hours. One side effect was that the vulnerabilities would be patched within three days.For container images, an internal “golden image” was used by all products. The downstream images that used this were built automatically. Trivy (a scanning tool) was also embedded into CI.Regarding FIPS, there is a long processing time for NIST (government agency) to validate new kernels and cryptographic libraries. Thus, Palantir cannot used features offered by new versions of the kernel.Regarding service-to-service communication, Cilium CNI is used for k8s clusters. IPSec encryption in Cilium ensures FIPS validated encryption between pods. Cilium also has powerful network policy primitives which made it easier to adhere to FedRAMP standards.Regarding Ingress/ Egress traffic, NGINX+ provides FIPS validation but there were performance problems encountered by Palantir. The decision was made to switched to Envoy, which is an open sourced service proxy designed for cloud native applications. BoringSSL with FIPS configured was used as the TLS provider.Regarding Host Intrusion Detection System, originally OSQUERY tool was used. However, it did not integrate well with k8s so all the pods showed up as similar processes. The decision was made to switch to Isovalent Tetragon which was an eBPF tool that integrated well with k8s.Continuing ChallengesThere are more challenges not solved by CNCF technologies out of the box. To solve this, Palantir created Apollo and FedStart which helps companies to deploy software for the federal environment. Change Management: Security relevant changes must be approved by authorized US person. Combining compliance checks with other automation tools is challenging. Policy-based rollouts allow CICD while enforcing approvals from the US persons. Process Controls: A lot of FedRAMP controls are not technical, but more of procedure-related. Standardized infrastructure provided by Palantir means that the companies can just follow the provided templates for the documentation rather than creating from scratch. Vulnerability Management: Palantir manages minimized images to reduce exposures to CVEs." }, { "title": "Learning Points: Snowflake Iceberg, Streaming, Unistore", "url": "/posts/snowflake-new-streams-talk/", "categories": "Tech", "tags": "Data-Engineering, System-Design, Learning-Points", "date": "2024-01-21 02:20:00 +0000", "snippet": "This video is about Snowflake Iceberg Tables, Streaming Ingest and Unistore. The presenters are N.Single, T.Jones and A.Motivala as part of the Database Seminar Series by CMU Database Group.Problem...", "content": "This video is about Snowflake Iceberg Tables, Streaming Ingest and Unistore. The presenters are N.Single, T.Jones and A.Motivala as part of the Database Seminar Series by CMU Database Group.Problems with Traditional Data LakesTraditional data lakes use file systems as the metadata layer. For example, data for each table is organized in a directory. Partitioning is implemented through nested directories. Using directories as database tables cause problems. Not easy to provide ACID guarantees. Multiple partition inserts were not atomic. Tools may directly access the file systems without consistent metadata updates. Schema evolution was very error prone. No clear way for access control.Apache Iceberg Describes how to perform updates to the table. Specification to achieve snapshot isolation. File memberships are defined to a snapshot. Easier schema evolution with Iceberg metadata.Snowflake ArchitectureTable metadata and data are stored as Parquet files on customers’ bucket. +-------------------------------------------------+ Cloud services | Authentication and Authorization | +-------------------------------------------------+ | Infra Manager | Optimizer | Transaction Manager | +-------------------------------------------------+ | Metadata Storage (Customer's Bucket) | +-------------------------------------------------+ +-------------------+ +-------------------+ Compute | Warehouse | | Warehouse | +-------------------+ +-------------------+ +-------------------------------------------------+ Storage | Data (Customer's Bucket) | +-------------------------------------------------+Customers will have to provide Snowflake External Volumes on any the cloud providers with access credentials. Data and metadata files are written to the External Volume.Metadata GenerationSnowflake has its own files to store snapshot metadata originally. To support Iceberg format, each table commit requires generation of both Iceberg metadata and internal Snowflake metadata.The generation of additional metadata files (Iceberg) increases query latency significantly. Thus Iceberg metadata files are generated on the background at the same time.When Snowflake metadata files are generated, the transaction is considered commited. If the server crashes before Iceberg metadata is generated, the request would come to the new Snowflake server and the Iceberg metadata will be generated on the fly.How Spark Accesses IcebergThe Iceberg SDK accesses a catalog which returns the location of metadata files in customers’ buckets. Then the SDK interprets the metadata files and returns the locations of data files in an API to Spark.Spark ---&gt; Iceberg SDK ---&gt; 1. Catalog (Hive, Glue) | | | -----------&gt; 2. Storage (Snapshot Metadata) | ------------&gt; 3. Data FilesSnowpipe StreamingBefore this feature, the original Snowpipe did continuous copying from a bucket to a table behind the scenes, in batches. However, there was no low latency, high throughput, in-order processing feature. Snowpipe Streaming provides: Ingestion to tables over HTTPs. Exactly once (?) and per-channel ordering guarantees. Low latency, queryable after seconds. High throughput, GB/s supported. Low overhead for configuration.New concepts include: Channel - Logical partition that represents a connection from a single client to a destination table. Client SDK - Accepts bytes from application, writes data to cloud storage as blobs and registers them to Snowflake. Mixed table - Contains both BDEC (Chunks of Arrow format) that is written by the client SDK and Snowflake’s propriatory FDN format. In the background, the BDEC files are rewritten into FDN format. The rewriting process is transparent to the users as queries can be done on the mixture of BDEC and FDN files. However, the rewriting process implies additional compute which will be charged to the customer.The implementation details: User code uses the Snowpipe Streaming Client SDK to open a Channel and write rows in the Channel. Client SDK writes BDEC files to the Streaming Ingest’s internal storage (Blobstore). Note that FDN files exist in the same Blobstore. Client SDK registers the blob via REST API to Snowflake’s Frontend node. Frontend node fans out per-table registration requests to the Snowflake’s Commit Service and provides a progress update to the client SDK. The Commit Service validates and deduplicates chunks per-table in memory. Then it commits by changing table version references to the new Arrow chunks (BDEC). Snowpipe creates regular FDN files from BDEC files. At this point, queries would reflect the newly added data in BDEC files.UnistoreSnowflake’s product for combining transactional and analytical workload on one platform.A new table type that works with existing snowflake tables, supports transactional features such as unique keys, referential integrity constraints and cross domain transactions.CREATE HYBRID TABLE CustomerTable { customer_id int primary key, full_name varchar(256), ...}" }, { "title": "Learning Points: Scaling Monitoring from Prometheus to M3", "url": "/posts/scaling-monitoring-with-m3/", "categories": "Tech", "tags": "System-Design, Learning-Points", "date": "2024-01-20 03:30:00 +0000", "snippet": "I enjoy watching 45 minutes to 1 hour long technical talks at conferences. Unfortunately, I am not retaining the knowledge as long as I would like to. From now on, I am going to try summarizing my ...", "content": "I enjoy watching 45 minutes to 1 hour long technical talks at conferences. Unfortunately, I am not retaining the knowledge as long as I would like to. From now on, I am going to try summarizing my takeaways for each videos to improve my own retention.This video is about scaling monitoring from Prometheus to M3 at Databricks presented by YY Wan and Nick Lanham.ContextPrometheus based monitoring system is used in Databricks since 2016. Most internal services run on Kubernetes and Spark workloads run in VMs in customer environments. PromQL is widely used by engineers.In each region, there are two Prometheus servers, Prom-Normal and Prom-Proxied. Prom-Normal scrapes metrics from internal services in k8s pods. Metrics from external services are pushed by Kafka to the Metrics Proxy Service (on k8s). Prom-Proxied scrapes metrics from the Metrics Proxy Service. Having two servers also means metrics can be sharded logically (internal/external) as all the metrics would not fit on one. Disks are attached to each Prometheus server to store metrics.Globally, there is a Prometheus server that contain a subset of metrics federated from all the regions.Users interact with the monitoring system in two ways: alerting and querying. Regional Prometheus servers issue alerts to the Alert Manager Service which notifies engineers via PagerDuty. Users also query regional or global servers for insights.Problem50 regions across multiple cloud providers with 4 million VMs of Databricks services and Spark workers. The global Prometheus server was huge. Disk usage of 4TB. Big queries not completing due to frequent OOMs. Short retention period, only 15 days. Users have sharded view of metrics (Prom-Normal, Prom-Proxied). Strict metrics whitelist to keep the servers running. Metrics are lost during restarts as it takes a while to replay the large volume of logs per server.RequirementsMust: High metrics volume. 90 days retention. PromQL compatible. Global view of metrics. High availability setup.Nice to have: Good maintenance story, no metrics gap. Open source. Battle tested.Why M3?Why M3 solves the problem for Databricks: Horizontally scalable. High availability with multi-replica setup. Exposes Prometheus API query endpoint. Global querying feature. Battle-tested at Uber production environment. Exists k8s operator for automated deployments.M3 ArchitectureApplication --- M3 collector | M3 aggregator | M3DB --- M3 query --- Grafana M3DB is a distributed time-series database, optimizations include time series compression. Sharding is also in-built. M3 Query is a stateless query server that accepts M3QL or PromQL. Coordinates with servers in other regions to achieve global queries. M3 Coordinator provides APIs to read and writing to M3DB. It also acts as a bridge with Prometheus. M3 Aggregator provides streaming aggregation of time series data. It reduces cardinality and/or datapoint resolution to reduce the volume of data stored.Initial PlanProm-Normal and Prom-Proxied remote-write data in M3DB instead of local disks. For regional query, users only need to interact with regional M3DB instead of thinking about sharded view of Prom-Normal and Prom-Proxied metrics. M3 Query server would also provide global view without requiring another global Prometheus server. Least amount of work.However, remote-writes by only two Prometheus servers could not achieved at a scale that Databricks required.Small Components to Replace Prom-Normal and Prom-ProxiedMore servers would achieve higher write throughput into M3DB.To replace Prom-Normal, multiple Grafana Scrape Agents scrape metrics from internal services and write to M3DB.To replace Prom-Proxied, Metrics Proxy Service directly writes to M3DB. Note that this service is already made up of multiple servers. This reduces end-to-end latency of external metrics too.Update AlertingOriginally, the alerting rule configurations are used in Prometheus servers to issue alerts to Alert Manager Service.Databricks built its own rule engine that takes the same configurations and interacts with M3DB and Alert Manager Service.Noisy Neighbor IssuesM3 Coordinators were having noisy neighbor issues. If users submit heavy queries, the coordinators would not be able to serve the write paths from Metrics Proxy Service and Grafana Scrape Agents.To solve this, M3 Coordinators were separately deployed for read and writes. CPU-heavy machines for write-coordinators and Memory-heavy machines for read-coordinators.Monitoring the M3 Monitoring SystemVanilla Prometheus servers that scrape M3 related components. Metrics retention period is short but it is sufficient for the use case.Global Prometheus server to federate metrics for all the Prometheus server.Final Architecture" }, { "title": "Learning Points: Kubernetes Networking Deep Dive", "url": "/posts/kubernetes-networking-intro-and-deep-dive/", "categories": "Tech", "tags": "Networking, System-Design, Learning-Points", "date": "2024-01-20 03:30:00 +0000", "snippet": "This video is about how networking works in Kubernetes by Bowei Du and Tim Hockin from Google.Networking APIs Exposed by Kubernetes Service, Endpoint: Service registration and discovery. Ingress:...", "content": "This video is about how networking works in Kubernetes by Bowei Du and Tim Hockin from Google.Networking APIs Exposed by Kubernetes Service, Endpoint: Service registration and discovery. Ingress: L7 HTTP routing. Gateway: Next-gen HTTP routing and service ingress. NetworkPolicy: Application firewall.Pod Networking ModelAll pods can reach all other pods across nodes. The network drivers on each node and networking between pods are implemented by Kubelet CNI implementation.One implementation is using hosts as a router device while a routing entry is added for each pod. For example, Flannel host-gw mode and Calico BGP mode. Another implementation is using overlay networks where layer 2 frames are encapsulated into layer 4 UDP packets alongside a VxLAN header. For example, Flannel and Calico VxLAN mode.Service API ImplementationPod IP addresses are ephemeral. Service API exposes a group of pods via one IP (ClusterIP). This is how the API works: A client pod sends a DNS query of to KubeDNS service. The KubeProxy sends the query to a KubeDNS pod which returns the ClusterIP. The client sends a packet to the ClusterIP. The KubeProxy intercepts the packet and sends it to one of the server pods. If the server pod goes down, the client can retry with the same ClusterIP.kind: ServiceapiVersion: v1metadata: name: my-service namespace: defaultspec: selector: app: my-app ports: - port: 80 # for clients - targetPort: 9376 # for backend podsKubeProxy runs on every node in the cluster. It uses iptables, IPVS or userspace options to proxy traffic from pods.KubeProxy control plane accumulate changes to Endpoints and Services, then updates rules in the node. In the data plane, the sending KubeProxy recognizes ClusterIP/port and rewrites packets to the new destination (DNAT). The recipient KubeProxy un-DNAT the packets.To disambiguate, CNI ensures the Pod IPs work. KubeProxy redirects ClusterIP to Pod IP before sending over the network.Endpoint API ImplementationEndpoint objects are a list of IPs behind a Service. An Endpoint Controller manages them automatically.When a Service object is created, an Endpoint object is created that has a mapping of service name to pod addresses and ports. This object is fed into the rest of the system such as KubeDNS and KubeProxy.Ingress API ImplementationHTTP proxy and L7 routing rules that targets a service for each rule. Kubernetes define the API but implementations are all third party.Unlike the Ingress API, Service-type load balancers only work at L4 level.Ingress { hostname: foo.com paths: - path: /foo service: foo-svc - path: /bar service: bar-svc}Deep Dive: NodeLocal DNSDNS resource cost is high. There are more microservice addressed by names and more application libraries tending to use DNS names. The solution is to run a DNS cache on every node.The NodeLocal DNS implementation is deployed on each node as a Daemonset.Dummy network interface is created that binds to ClusterIP address of KubeDNS. Linux NOTRACK target is added with KubeDNS ClusterIP before any KubeProxy rules. This ensures that NodeLocal DNS can process the packets without them reaching KubeProxy.A watcher process removes the NOTRACK entries in the event that NodeLocalDNS fails. This defaults back to the original KubeDNS infrastructure.Deep Dive: EndpointSliceEndpoint objects are stored in the Etcd database. When one pod IP changes, the entire object has to be redistributed to all the KubeProxy. If Endpoint objects are large, it may also hit the maximum storage limit in Etcd.The solution is to represent one original Endpoint object with a set of EndpointSlice objects. A single update to pod IP will only require redistribution of one EndpointSlice object.The EndpointSlice controller slices from a Service object to create EndpointSlice objects.Interesting optimization problem: Keep number of slices low. Minimize changes to slices per update. Keep the amount of data sent low." }, { "title": "CMU MCDS Course Reviews", "url": "/posts/cmu-mcds-course-reviews/", "categories": "School", "tags": "CMU", "date": "2024-01-19 02:00:00 +0000", "snippet": "Over the last 1.5 years, I studied Master of Computational Data Science (MCDS) at Carnegie Mellon University. Inspired by blogs such as fanpu.io and wanshenl.me, I am going to outline my experience...", "content": "Over the last 1.5 years, I studied Master of Computational Data Science (MCDS) at Carnegie Mellon University. Inspired by blogs such as fanpu.io and wanshenl.me, I am going to outline my experiences for each course to hopefully help future students.Summer 2022 15513 - Introduction to Computer SystemsAs someone who does not have a systems background, this course was transformational. It started with the foundation of how hardware instructions work and build up to computer memory, processes and threads. I took it with a full time internship but fortunately the class was asynchronous. I watched the lectures on weekday nights and worked on the projects on the weekends. The workload was manageable except for weeks of Malloc Lab which were heavier. For graduate students, this course was only 6 units so the tuition fee was only half the normal cost. Doing well in this course is compulsory for MCDS students who wish to choose the Systems concentration. Notable projects include: Bomb lab: Reading through assembly code to decode its purpose. Malloc lab: Building a memory allocator where throughput and memory fragmentation is graded. Tsh lab: Building a simple shell to manage processes. Proxy lab: Web server to serve and cache static files concurrently. Fall 2022 10601 - Introduction to Machine LearningThis course focused on how each type of machine learning models work from the ground-up, aka all mathematics. There were 3 written exams and homeworks with written and programming components. I spent more effort on the written components and preparing for the exams. The programming contained straightforward instructions and were not too complex. For students with more machine learning background, the course code for the PhD level version is 10701. 11631 - Data Science SeminarA compulsary course for MCDS students. The focus was on reading and writing scientific papers. Every week students critique data science papers and submit paper summaries. There was also a component where students read the papers of the senior MCDS batch and critique them. Personally, I felt that the selected readings were too focused on the human-computer-interaction concentration of MCDS and did not include the other two concentrations (analytics and systems). The workload was the lowest compared to the other courses in the semester. 11637 - Foundation of Computational Data ScienceThere were some overlaps with 10601 in terms of the machine learning content. However, this course included the surrounding infrastructure around a machine learning model such as data processing and deploying the models. The course was asynchronous where students read the lectures on their own time which provided flexibility in terms of scheduling. The projects were time-consuming but not as technically complex as the systems related courses. Personally, I did not learn much more than the basic ML engineering experiences that I already had from internships. 15645 - Database SystemsThe most exciting course for the semester. I got to build components of the Bustub database system. The professor, Andy Pavlo, was also very engaging which led to a lot of participation during class. The contents ranged from the low level concepts such as how the database interacts with the disk and optimizing query plans to higher level concepts such as how distributed databases work. The projects were in C++ and emphasized on multi-threading support. Optimizations could lead to higher ranks on the leaderboard and extra credits. As someone without any C++ experience, this course had the most difficult projects for the semester but after investing significant amount of time, it was still possible to achieve top 10 leaderboard ranking. My lecture notes can be found here. Notable projects include: Storage Index: B+ tree data structure with concurrent protocols. Query Execution: Iterator query processing model and query plan optimization. Concurrency Control: Lock-based concurrency control with different isolation levels. Spring 2023 05839 - Interactive Data ScienceThe course was about visualization techniques in data science. The students have to read the lecture slides before attending the lectures afterwards. Contents included drawing charts, interpreting them and using frameworks to deploy them. There was also a final project where students could choose almost any dataset and draw visualizations in Streamlit. In my opinion, this was not sufficiently challenging for a graduate level course that was compulsory for MCDS students. A more appropriate audience would be freshmen students with not a lot of programming background. 11634 - Capstone Planning SeminarMCDS students would have to work on a capstone project spanning two semesters. This course covered how to produce documentations for a data science project and also helped the students to match with project mentors. Generally, the contents provided a good structure to plan out the capstone projects from scratch. However, a lot of documentation was required for submission with tight deadlines and this took some time away from the actual discussions of the projects. 15640 - Distributed SystemsIt was an introductory course on how to design and implement scalable distributed systems. The Spring version was taught in C and Java unlike the Fall version which was in Golang. I was fortunate to be taught by Professor Satya who implemented Andrew File System. The projects included implementing RPC, a distributed caching system, a scalable web service and the two-phased commit protocol. Some ideas overlap with 15645 such as transactions and logging. The projects were easier too. I spent about half the time on each project compared to 15645. My lecture notes can be found here. 15719 - Advanced Cloud ComputingRather than how to use cloud technologies, the course covered how each type is implemented. I really liked the course as now I have a clearer understanding on the abstractions behind services on the cloud providers. For a better understanding, I really recommend going through the original papers provided as the compulsory reading materials as the lecture notes condenses them too much. It would also be helpful for system design interviews as the papers explains the complexities behind each system very well. In terms of assessment, the exams tested the application of each concept in scenarios. The projects on Spark were challenging but the other projects on Terraform and Kubernetes were easy for a “systems course”. For MCDS students who took 15513, the course can be taken to replace 15619 - Cloud Computing.Fall 2023 11632 / 11635 - Data Science CapstoneThe time was meant for implementing the proposals that were submitted during 11634. Weekly standups had to be submitted in the form of videos and google forms. Every few weeks, the teams had to present the updates to Professor Nyberg. One recommendation is to be very particular about planning, especially to have roadmaps with timelines and task assignments. The workload varies extremely widely across each team. The same mentors are usually involved with multiple batches of MCDS students so I would highly recommend talking to the previous teams beforehand. 15641 - Computer NetworksOne of my favourite courses in CMU. Professor Sherry is a very friendly and engaging person which led to a lot of participation in class. The contents included a deep dive into each layer of TCP-IP model and network security. Even though I took a networking class before during undergraduate, I still found the projects exciting to work on. People could choose to work in pairs, but it was still very manageable to work on them alone. It was more challenging than 15640 but less than 15645. Projects were in C and included: Mixnet: Distributed spanning tree with link state routing to transport frames across nodes. TCP: Implementing features of TCP over UDP sockets. Writing 3-way handshake, flow control and congestion control from scratch really made the concepts stuck in my head. HTTP: Single-threaded HTTP server with Linux epoll to handle multiple clients. 17663 - Programming Language PragmaticsThe course spent about 60% on formally proving the properties of programming languages and 40% on the compiler implementation. As someone with little theory background, this was the toughest course for the semester. The proofs had to be written in SASyLF which checked the correctness during compilation. This short feedback loop made the learning process easier. The compiler was to be written in OCaml and transforms a subset of TypeScript into WebAssembly code. As a systems student, this component was a lot more engaging to me but I can now also appreciate the importance of formal reasoning. From my impression, it seemed that the other undergraduate students with more theory background found the course relatively easy." }, { "title": "Data Structures Behind Git", "url": "/posts/data-structures-behind-git/", "categories": "Tech", "tags": "Git", "date": "2022-05-13 00:22:00 +0000", "snippet": "The underlying data structure of a Git repository is just a directed acylic graph (DAG). Not only the core idea is simple, the implementation can be easily inspected in the .git directory. Let’s br...", "content": "The underlying data structure of a Git repository is just a directed acylic graph (DAG). Not only the core idea is simple, the implementation can be easily inspected in the .git directory. Let’s break it down.If Git is a graph, what are the “nodes”?There are three types of “nodes”, also known as Git Objects - Blobs, Trees and Commits. The article will run through an example usage of Git so that we can observe how each of them is created.Empty Git repositoryAfter initializing an empty Git repository, the .git directory is shown below. For the rest of the article, our focus will be on the .git/objects directory..|____branches|____config|____description|____HEAD|____hooks| |____applypatch-msg.sample| |____commit-msg.sample| |...|____info| |____exclude|____objects &lt;--- Our focus| |____info| |____pack|____refs| |____heads| |____tagsGit Object - BlobWe will create a new file and add it to staging.echo \"Hello World\" &gt; hello.txtgit add hello.txtNow we can find a new directory and file in .git/objects..|____objects| |____55| | |____7db03de997c86a4a028e1ebd3a1ceb225be238| |____info| |____pack...Note that the concatenation of the directory name and file name is a 20-byte hash digest.If we inspect the file, we will see that it is not a human-readable format.cat .git/objects/55/7db03de997c86a4a028e1ebd3a1ceb225be238xKOR04bH/IAIThis is because Git stores content in a compressed binary format. If we uncompress with the appropriate algorithm, the contents can be seen as below. This Git Object is indicated to be a 12-byte Blob with data being Hello World\\n.blob 12Hello WorldRemember the 20-byte hash digest? To produce it, Git has actually run SHA-1 on the uncompressed data shown above. In other words, a Blob is simply contents of one file and is identified by SHA-1 hash of its contents.Note that our Blob does not contain any information about the file name hello.txt.Inspecting Git ObjectsConvenient to us, Git provides APIs to inspect the compressed data in Git Objects.# Inspect the type of Git Objectgit cat-file -t 557db# Inspect the content of Git Objectgit cat-file -p 557db# Type of Git Objectblob# Content of Git ObjectHello WorldGit Object - TreeNow that we understand what a Blob is, let’s create a new commit.git commit -m \"First commit\"Looking at .git/objects again, there are two new Git Objects created..|____objects| |____55| | |____7db03de997c86a4a028e1ebd3a1ceb225be238| |____97| | |____b49d4c943e3715fe30f141cc6f27a8548cee0e &lt;-- New file 1| |____c5| | |____5df28adf8320cc4d15637b82e8a0b13422d955 &lt;-- New file 2...If we inspect 97b49 with cat-file, the Git Object type and its contents are shown below.# Type of Git Objecttree# Content of Git Object100644 blob 557db03de997c86a4a028e1ebd3a1ceb225be238 hello.txtIt can be seen that this particular Git Object is a Tree. More specifically, it has a pointer to a Blob with hash digest 557db while naming it hello.txt. It also states that the file has a 644 permission.In the example, the Tree has one Blob pointer but in reality it can have multiple Blob pointers and even other Tree pointers. In other words, a Tree simply contains pointers to other Git Objects and is identified by the SHA-1 hash of its contents.Excluding the file names from Blobs is an intentional optimization by Git. If there are two files with duplicate content but different names, Git’s representation will be multiple pointers pointing to the same Blob.Git Object - CommitThere is still one more Git Object. If we inspect c55df with cat-file, the results are shown below.# Type of Git Objectcommit# Content of Git Objecttree 97b49d4c943e3715fe30f141cc6f27a8548cee0eauthor yarkhinephyo &lt;yarkhinephyo@gmail.com&gt; 1652402598 +0800committer yarkhinephyo &lt;yarkhinephyo@gmail.com&gt; 1652402598 +0800It can be seen that a Commit contains a pointer to a single Tree encompassing the contents of the commit and other bookkeeping details (such as author and timestamp). Similar to other Git Objects, a Commit is also identified by the SHA-1 hash of its contents.Putting it together - Directed acyclic graphecho \"Hello World\" &gt; hello.txtgit add hello.txtgit commit \"First commit\"Considering all the pointers, the Git Objects resulting from these commands can be represented as a graph.DAG after first commit - Diagram by authorThis is essentially the data structure powering Git repositories, stored right in the .git/objects directory as compressed binary files.Adding a new commitLet’s see what happens with a new commit. We will modify hello.txt and add new_file.txt in the second commit.echo \"Bye\" &gt;&gt; hello.txtecho \"I love git\" &gt; new_file.txtgit add hello.txt new_file.txtgit commit -m \"Second commit\"If we look at the .git/objects directory and inspect the new Git Objects with cat-file tool, it is possible to manually update the graph.DAG after second commit - Diagram by authorThere are two interesting observations.First, the new Commit has a pointer to the parent Commit in its contents. This means that whatever is in the ancestor Commits affects the SHA-1 calculation of the new Commit. Therefore, as long as we have the SHA-1 calculation of the latest commit, the integrity of Git history can be verified.Second, a new Blob is created after hello.txt has been modified and a new Tree stores a pointer to it. This is because Git Objects are immutable. Whatever changes made in a new commit would not mutate the previous Git Objects and modify the SHA-1 calculations.Merkle DAGThis DAG where each node has an identifier resulting from hashing its contents is called Merkel DAG. This data structure also plays an important role in Web3 applications.Resources Advanced Git: Graphs, Hashes and Compression by InfoQ Git Merkle DAG by John Williams" }, { "title": "JVM Garbage Collectors - Serial, Parallel, CMS", "url": "/posts/jvm-garbage-collectors/", "categories": "Tech", "tags": "Java", "date": "2022-05-06 09:00:00 +0000", "snippet": "Depending on the resources available and the performance metric of an application, different Garbage Collectors (GC) should be considered for the underlying Java Virtual Machine. This post explains...", "content": "Depending on the resources available and the performance metric of an application, different Garbage Collectors (GC) should be considered for the underlying Java Virtual Machine. This post explains the main idea behind the garbage collection process in JVM and summarizes the pros and cons of Serial GC, Parallel GC and Concurrent-Mark-Sweep GC.Garbage-First GC (G1) is out-of-scope for this post as it works very differently from the other algorithms (and I still have not wrap my head around it). This post also assumes familiarity with heap memory.Garbage Collection AlgorithmsThese symbols will be used to illustrate the memory allocation in heap.o - unvisitedx - visited&lt;empty&gt; - freeMark-Sweep: The objects in the heap that can be reached from root nodes (such as stack references) are marked as visited. While sweeping the memory, the regions occupied by the unvisited objects are updated to be free. As there are likely to be less contiguous regions after a collection, external fragmentation is likely to occur.Marked | x |o| x | o |x|Sweeped | x | | x | |x|Mark-Sweep-Compact: After marking, the visited objects are identified and compacted to the beginning of the memory region. This solves the external fragmentation issue, but more time is required as objects have to be moved and references have to be updated accordingly.Marked | x |o| x | o |x|Sweeped | x | | x | |x|Compacted | x | x |x| |Mark-Copy: After marking, the visited objects are relocated to another region. This accomplishes compaction of allocated memory at the same step. However, the disadvantage is that there is a need to maintain one more memory region.Marked | x |o| x | o |x| |Copied | | x | x |x| |During parts of a garbage collection, all application threads may be suspended. This is called stop-the-world pause. Long pauses are especially undesirable in interactive applications.Generational Garbage Collection in JVMThe Weak Generational Hypothesis states that most objects die young.In JVM, heap memory is divided into two regions - Young Generation and Old Generation. Newly created objects are stored in the Young Generation and the older ones are promoted to the Old Generation. With this separation, GC can work more often in a smaller region where dead objects are more likely to be found.&lt;- Young Generation -&gt;+--------------------+--------------------+| Eden Space | |+----------+---------+ Old Generation || S0 | S1 | |+----------+---------+--------------------+Young Generation: The region is divided into Eden Space where new objects are created, and S0/S1 Space where the visited objects from each garbage collection can be copied to. Naturally, Mark-Copy algorithm is used.Old Generation: As there is no delimited region for the visited objects to be copied to. Only Mark-Sweep and Mark-Sweep-Compact algorithms can be used.Serial GCJVM option: -XX:+UseSerialGCThis option uses Mark-Copy for the Young Generation and Mark-Sweep-Compact for the Old Generation. Both of the collectors are single-threaded. Without leveraging multiple cores present in modern processors, the stop-the-world pauses are longer. The advantage is that there is less resource overhead compared to other options.Parallel GCJVM option: -XX:+UseParallelGC -XX:+UseParallelOldGCSimilary to Serial GC, this option uses Mark-Copy for the Young Generation and Mark-Sweep-Compact for the Old Generation. Unlike Serial GC, multiple threads are run for the respective algorithms. As less time is spent on garbage collection, there is higher throughput for the application.Concurrent-Mark-Sweep (CMS) GCJVM option: -XX:+UseParNewGC -XX:+UseConcMarkSweepGCFor the Young Generation, this option is the same as Parallel GC. For the Old Generation, this option runs most of the job in Mark-Sweep concurrently with the application. This means that the application threads continue running during some parts of the garbage collection. Hence this option is less affected by stop-the-world pauses compared to the other two, making it preferred for interactive applications.As at least one thread is used for garbage collection all the time, the application has lower throughput. Without compaction, external fragmentation may also occur. When this happens, there is a fallback with Serial GC but it is very time-consuming.Resources Java Garbage Collection by Plumbr Garbage Collection in Java by Ranjith" }, { "title": "Solving HTTP/1 Problems With HTTP/2", "url": "/posts/solving-http1-problems-with-http2/", "categories": "Tech", "tags": "Networking", "date": "2022-05-02 03:09:00 +0000", "snippet": "HTTP/2 has made our applications faster and more robust by providing protocol enhancements over HTTP/1. This post only focuses on the major pain points of HTTP/1 and how the new protocol has been e...", "content": "HTTP/2 has made our applications faster and more robust by providing protocol enhancements over HTTP/1. This post only focuses on the major pain points of HTTP/1 and how the new protocol has been engineered to overcome them. It is assumed that the reader is familiar with how HTTP/1 works.Problems with HTTP/1Head-of-line blocking: Browsers typically only allow 6 parallel TCP connections per domain. If the initial requests are not complete, the subsequent requests will be blocked.Unnecessary resource utilization: With HTTP/1, a single connection is created for every request, even if multiple requests are directed to the same server. As the server has to maintain states for each connection, there is an inefficient utilization of resources.Overhead of headers: Headers in HTTP/1 are in the human-readable text format instead of being encoded to be more space-efficient. As there are numerous headers in complex HTTP requests and responses, it can become a significant overhead.Binary Framing “Layer”In HTTP/2, there is a binary framing layer that exists between the original HTTP API exposed to the applications and the transport layer. The rest of TCP/IP stack is unaffected. As long as both client and server implements HTTP/2, the applications will also continue to function as usual.Request and response multiplexingIn HTTP/1, each HTTP request creates a separate TCP connection as shown below.| - Application - | - Transport - | request_1 --&gt; connection_1 request_2 --&gt; connection_2In HTTP/2, the binary framing layer breaks down each request into units called frames. These frames are interleaved and sent to the transport layer as application data. The transport layer is oblivious to the process and carries on with its own responsibilities. At the server’s end, the binary framing layer reconstruct the requests from the frames.| - Application ---------------- | - Transport - | | Binary Framing | request_1 ---&gt; frames ---&gt; connection_1 request_2 -/ HTTP/2 FramesTo be specific, each HTTP request is broken down into HEADERS frame and DATA frame/s. The names are self-explanatory. HEADERS frame include HTTP headers and DATA frame/s include the body.The diagram below shows the structure of a frame.+-----------------------------------------------+| Length (24) |+---------------+---------------+---------------+| Type (8) | Flags (8) |+-+-------------+---------------+-------------------------------+|R| Stream Identifier (31) |+=+=============================================================+| Frame Payload (0...) ...+---------------------------------------------------------------+HTTP/2 StreamsNotice that each HTTP/2 frame has an associated stream identifier which identifies each bidirectional flow of bytes. For example, all the frames in a single request-response exchange will have the same stream identifier.This means that when frames from different requests are interleaved with one another, the receiving binary framing layer can reconstruct them back into independent streams.Interleaving frames with different stream identifiersIn other words, the hierarchical relationship between connection, stream and frame can be represented as shown below.Logical relationship between frames and streamsHeader compressionAside from multiplexing HTTP requests over a single TCP connection, HTTP/2 also provides a mechanism for header compression. Instead of transporting textual data, both server and client maintain identical lookup tables to remember the headers that have been used. In the subsequent communication, only the pointers into the lookup table are sent over the network. Tests have show that on average, the header size is reduced around 85%-88%.LimitationHTTP/2 solves the head-of-line blocking problem from parallel TCP connections. However, this creates another problem at the TCP level. Due to the nature of TCP implementation, one lost packet can make all the streams wait until the packet is re-transmitted and received.HTTP/3 addresses this issue by communicating over QUIC (TCP-like protocol over UDP) instead of TCP.Resources A Brief History of SPDY and HTTP/2 by Ilya and Surma HTTP2 by High Performance Browser Networking" }, { "title": "Choosing Between UTF Encodings", "url": "/posts/choosing-between-utf-encodings/", "categories": "Tech", "tags": "Data-Engineering", "date": "2022-04-27 04:07:00 +0000", "snippet": "Have you occasionally chosen a character encoding such as UTF-8 during reading and writing files while wondering its purpose? I have! This post explains various UTF (Unicode Transformation Format) ...", "content": "Have you occasionally chosen a character encoding such as UTF-8 during reading and writing files while wondering its purpose? I have! This post explains various UTF (Unicode Transformation Format) algorithms such as UTF-8, UTF-16, UTF32 and how to choose between them.Unicode character setUnicode character set defines a unique number for almost all characters used in modern texts today. The standard ensures that given a number, also known as a code point, different softwares will decode it as the same character. Character Decimal Representation Code (Read Hexadecimal) A 65 U+41 B 66 U+42 我 25105 U+6211 😋 128523 U+1F60B The Unicode character set ranges from 0x0 to 0x10FFFF (21-bits range).UTF-32UTF stands for Unicode Transformation Format. It encodes integer code points into byte representations on a machine. For example, if 4 bytes are allocated to a character at each time, four-byte representations are shown below. Character Byte Representation (Read Hexadecimal) A 0x00000041 B 0x00000042 我 0x00006211 😋 0x0001F60B This is exactly what UTF-32 does. It pads every code point with zeros into 32 bits. This is more than sufficient for the 21-bits range of Unicode character set.However, the approach is space-inefficient. For example, if there are only English letters in a document (U+41 to U+7A), only one byte is necessary to represent each character. However, UTF-32 will still pad with three bytes to form four-byte representations, resulting as 300% increase in storage.UTF-16UTF-16 mitigates the problem by representing U+0 to U+FFFF with two bytes and U+10000 to U+10FFFF with four bytes.Characters from almost all modern languages are found in the first 216 code points (See Basic Multilingual Plane). If a document only contains these code points, UTF-16 will mainly use two-byte representations instead, meaning storage is cut by 50% from using UTF-32.To represent larger code points, UTF-16 employs a concept called surrogate pairs. High surrogates are code points from U+D800 to U+DBFF and low surrogates are code points from U+DC00 to U+DFFF. There are no character mappings at these ranges and they only have meaningful representations when paired. The example below may present a clearer picture.High surrogate --&gt; U+D800 to U+DBFF --&gt; 110110 concat with any 10 bitsLow surrogate --&gt; U+DC00 to U+DFFF --&gt; 110111 concat with any 10 bitsCharacter: 😋Unicode : U+1F60BBinary : 0b11111011000001011Binary padded 20-bits: 0b00011111011000001011 &lt;--- A --&gt;&lt;--- B --&gt; (10 bits) (10 bits)High surrogate: 110110 concat A = 1101100001111101 (16 bits)Low surrogate : 110111 concat B = 1101111000001011 (16 bits)If a decoder sees a two-byte representation starting with 110110 or 110111 bits, it can infer that this is part of a surrogate pair and immediately identify the other surrogate. The binary representation of the original character can be reconstructed afterwards.UTF-8ASCII characters compose the first 27 code points. Most of the time when coding or writing English articles, you may mostly end up using these characters. As these code points can be represented with one byte, two-byte representations of UTF-16 still results in wasted storage.Depending on the range of Unicode character set, UTF-8 uses one, two, three or four-byte representations. The encoding pseudocode is shown below.if code point &lt; 2^7 # Covers ASCII pad with zeros till 8 bits 1st byte = 8 bitselse if code point &lt; 2^11 # Covers other Latin alphabets pad with zeros till 11 bits # (5 + 6) 1st byte = \"110\" concat 5 bits 2nd byte = \"10\" concat 6 bitselse if code point &lt; 2^16 # Covers Basic Multilingual Plane pad with zeros till 16 bits # (4 + 6 + 6) 1st byte = \"1110\" concat 4 bits 2nd byte = \"10\" concat 6 bits 3rd byte = \"10\" concat 6 bitselse if code point &lt; 2^21 # Covers 21-bit Unicode range pad with zeros till 21 bits # (3 + 6 + 6 + 6) 1st byte = \"11110\" concat 3 bits 2nd byte = \"10\" concat 6 bits 3rd byte = \"10\" concat 6 bits 4rd byte = \"10\" concat 6 bitsAs texts encoded in ASCII never appear as multi-byte sequences, UTF-8 can be used to decode it directly. The backward compatibility is one of the reasons why it has been adopted at a large scale.How to choose between UTF-8, UTF-16, UTF-32If backward compatibility to ASCII is preferred and most characters are English text, UTF-8 is a good choice.If most characters are from non-English languages, UTF-16 is preferred because it uses two-byte representations for Basic Multilingual Plane as compared to UTF-8 which uses three-byte representations.UTF-32 is rarely used but in theory, the fixed-width encoding without transformations allows faster encoding and decoding of characters.Resources UTF-8 vs UTF-16 vs UTF-32 on StackOverflow How UTF-16 encodes 21-bit unicode Unicode Encoding! by EmNudge" }, { "title": "How the Same-Origin Policy Mitigates CSRF", "url": "/posts/how-the-same-origin-policy-mitigates-csrf/", "categories": "Tech", "tags": "Networking", "date": "2022-04-20 01:30:00 +0000", "snippet": "What is CSRF?A cross site request forgery (CSRF) attack occurs when a web browser is tricked into executing an unwanted action in an application that a user is logged in.For example, User A may be ...", "content": "What is CSRF?A cross site request forgery (CSRF) attack occurs when a web browser is tricked into executing an unwanted action in an application that a user is logged in.For example, User A may be logged onto Bank XYZ in the browser which uses cookies for authentication.Let’s say the a transfer request like this -GET http://bank-zyz.com/transfer?from=UserA&amp;to=UserB&amp;amt=100 HTTP/1.1Then a malicious actor can embed a request with similar signatures inside an innocent looking hyperlink.&lt;a href=\"http://bank-xyz.com/transfer?from=UserA&amp;to=MaliciousActor&amp;amt=10000 HTTP/1.1&gt;&lt;/a&gt;If User A clicks on the hyperlink, the web browser sends the request together with the session cookie.The funds are then unintentionally transferred out of User A’s account.When the same-origin-policy does not help with CSRFThe same-origin policy prevents a page from accessing results of cross domain requests.It prevents a malicious website from accessing another website’s resources such as static files and cookies.Even though the policy prevents cross-origin access of resources, it does not prevent the requests from being sent.In the Bank XYZ example, a GET request with relevant cookies triggers the server to transfer the funds before returning the 200 OK response.As shown in the diagram below, the same-origin policy only prevents the access of resouces, which in this case is reading the HTTP response.Since the request (with cookies) can still be sent, the hyperlink can still trigger the transfer of funds.Image by Author - The policy would only prevent a cross-origin access of HTTP response (Step 3)Note: For more complex HTTP requests, a preflight OPTIONS request is sent beforehand to check for relevant CORS headers.In that scenario, an unexpected cross-origin request will not reach Bank XYZ’s website at all.How the same-origin-policy actually mitigates CSRFTo prevent CSRF, Bank XYZ can generate an unpredictable token for each client which is validated in the subsequent requests.For example, a hidden HTML field can allow the token to be included in subsequent form submissions.&lt;input type=\"hidden\" name=\"csrf-token\" value=\"CIwNZNlR4XbisJF39I8yWnWX9wX4WFoz\" /&gt;Other websites running in User A’s browser do not have access to the form field due to the same-origin-policy.So malicious scripts from other origins can no longer make the same request to transfer funds.Resources StackOverFlow on why SOP is not enough not prevent CSRF CSRF by Imperva" }, { "title": "Git Submodules Cheatsheet", "url": "/posts/git-submodules-cheatsheet/", "categories": "Tech", "tags": "Git", "date": "2022-04-16 16:30:00 +0000", "snippet": "Git Submodules allows one Git repository to be a subdirectory of another. I keep forgetting the commands so I have created a 2-minute refresher for my future reference.Adding a submoduleTo add a su...", "content": "Git Submodules allows one Git repository to be a subdirectory of another. I keep forgetting the commands so I have created a 2-minute refresher for my future reference.Adding a submoduleTo add a submodule to a project, run the command as shown below. Git will clone the submodule to the path provided and create a new .gitmodules file to store the information.git submodule add &lt;remote-url&gt; &lt;path-to-module&gt;Note that the &lt;path-to-module&gt; is now tracked by the parent repository as a commit ID instead of a subdirectory of contents. Treat it as a file for all practical purposes.git add &lt;path-to-module&gt; .gitmodulesgit commit -m \"Added submodule\"Pushing an updated submoduleOnly the submodule’s commit ID is inspected by the parent repository. When the submodule’s commit is modified, the parent repository will react similarly to how a file has been modified. Add the modified “file” to staging and commit as usual.git add &lt;path-to-module&gt;git commit -m \"Updated submodule\"Pulling an updated submoduleAfter pulling changes from the parent repository, only the submodule’s tracked commit ID will be updated, not its contents. Manually update the contents of the submodule to synchronize with the updated commit ID.# This updates the commit IDs of submodulesgit pull origin main# Update the contents of the submodulesgit submodule update --init --recursiveCloning a repository containing submodulesAdd a --recursive flag.git clone --recursive &lt;module&gt;Resources Git Tools Submodules by Git Scm" }, { "title": "Primer to Scale-invariant Feature Transform", "url": "/posts/primer-to-sift/", "categories": "Tech", "tags": "Data-Science", "date": "2022-04-13 01:30:00 +0000", "snippet": "Scale-invariant Feature Transform, also known as SIFT, is a method to consistently represent features in an image even under different scales, rotations and lighting conditions. Since the video ser...", "content": "Scale-invariant Feature Transform, also known as SIFT, is a method to consistently represent features in an image even under different scales, rotations and lighting conditions. Since the video series by First Principles of Computer Vision covers the details very well, the post covers mainly my intuition. The topic requires prior knowledge on using Laplacian of Gaussian for edge detection in images.Why extract features?Image by First Principles of Computer VisionConsider the two images. How can the computer recognize that the object in the left is included inside the image on the right? One way is to use template-based matching where the left image is overlapped onto the right. Then some form of similarity measure can be calculated as it is shifted across the right image.Problem: To ensure different scales are accounted for, we would need templates of different sizes. To check for different orientations, we would need a template for every unit of angle. To overcome occlusion, we may even need to split the left image into multiple pieces and check if each of them matches.For the example above, our brains recognize the eye and the faces to locate the book. Our eyes do not scan every pixel, and we are not affected by the differences in scale and rotation. Similarly it will be great if we can 1) extract only interesting features from an image and 2) transform them into representations that are consistent across different scenes.Good requirements for feature representationPoints of Interest: Blob-like features with rich details are preferred over simple corners or edges.Insensitive to Scale: The feature representation should be normalized to its size.Insensitive to Rotation: The feature representation should be able to undo the effects of rotation.Insensitive to Lighting: The feature representation should be consistent under different lighting conditions.Blob detection - Scale-normalized points of interestImage from Princeton CS429 - 1D edge detectionIn traditional edge detection, a Laplacian operator can be applied to an image through convolution. Edges can be identified from the ripples in the response.Image from Princeton CS429 - 1D blob detectionIf multiple edges are at the right distance, there will be a single strong ripple caused by constructive interference. If this response is sufficiently strong, the location is identified as a blob representing a feature. Intuitively, complex features will be chosen compared to simple edges as constructive interferences cannot be produced by single edges.From the same diagram, we can also see that not all collection of edges result in singular ripples with a particular Laplacian operator. By increasing the sigma (σ) of the Laplacian (making the kernel “fatter”), the constructive interference will occur when edges are further apart. If we apply the Laplacian operators many times with varying σ’s, blobs of different scales can be identified each time.Image from Princeton CS429 - Increasing σ to identify larger blobsWait but if the σ is larger, the Laplacian response will be weaker (shown above). Intuitively, if the responses by larger blobs are penalized for their sizes. Does that means the selected features will be mostly tiny?Image from Princeton CS429 - Normalized Laplacian of the Gaussian (NLoG)We solve this by multiplying the Laplacian response with σ2 for normalization. (This works out because the Laplacian is the 2nd Gaussian derivative) Intuitively, this means that the response now only indicates the complexity of the features without any effect from their sizes.3 x 3 x 3 kernels to find local extremasImagine the Laplacian response represented as a matrix with x-y plane for image dimensions and z axis for various σ. We can slide an n x n x n kernel to find the local extremas. The resulting x-y coordinates would represent the centers of the blobs and σ would correspond to their sizes.With this technique, blobs can be extracted to represent complex features with the sizes normalized.Countering the effects of rotationImage from Princeton CS429To assign an orientation to each feature, it can be divided into smaller windows as shown above. Then the pixel gradient for each window can be computed to produce a histogram of gradient directions. The most prominent direction can be assigned as the principle orientation of the feature.Image by AuthorIn the example above, blobs are identified in both images representing the same feature. The black arrows are the principle orientations. After rescaling the blob sizes with the corresponding σ’s, the effect of rotation is eliminated by aligning with respect to the principle orientations.Countering the effects of lighting conditionsImage from Princeton CS429 - Pixels to SIFT descriptorsInstead of comparing each blob directly (pixel-by-pixel), we can produce a unique representation that is invariant to lighting conditions. As shown above, the image can be broken into smaller windows (4 x 4) where each histogram of the gradients is computed. If each histogram only consider 8 directions, there will be 8 dimensions per window. Even with only 16 windows per blob, each feature representation will be of 128 dimensions (16 x 8) which can be robust.These feature representations are known more formally as SIFT descriptors.ConclusionImage from OpenCV documentationFor matching images, SIFT descriptors in two images can be directly compared against one another through similarity measurements. If a large number of them matches, it is likely that the same objects are observed in both images. In practice, nearest neighbor algorithms such as FLANN are used to match the features between images.Resources SIFT Detector by First Principles of Computer Vision Feature Detectors and Descriptors - Princeton CS429" }, { "title": "Intuition Behind the Attention Head of Transformers", "url": "/posts/intuition-behind-the-attention-head/", "categories": "Tech", "tags": "Data-Science", "date": "2022-04-09 06:20:00 +0000", "snippet": "Even as I frequently use transformers for NLP projects, I have struggled with the intuition behind the multi-head attention mechanism outlined in the paper - Attention Is All You Need. This post wi...", "content": "Even as I frequently use transformers for NLP projects, I have struggled with the intuition behind the multi-head attention mechanism outlined in the paper - Attention Is All You Need. This post will act as a memo for my future self.Limitation of only using word embeddingsConsider the sequence of words - pool beats badminton. For the purpose of machine learning tasks, we can use word embeddings to represent each of them. The representation can be a matrix of three word embeddings.If we take a closer look, the word pool has multiple meanings. It can mean a swimming pool, some cue sports or a collection of things such as money. Humans can easily perceive the correct interpretation because of the word badminton. However, the word embedding of pool includes all the possible interpretations learnt from the training corpus.Can we add more context to the embedding representing pool? Optimally, we want it to be “aware” of the word badminton more than the word beats.My intuition behind the self-attention mechanismConsider that matrix A represents the sequence - pool beats badminton. There are three words (rows) and the word embedding has four dimensions (columns). The first dimension represents the concept of sports. Naturally, we expect the words pool and badminton to have more similarity in this dimension.Diagram by AuthorA = np.array([ [0.5, 0.1, 0.1, 0.2], [0.1, 0.5, 0.2, 0.1], [0.5, 0.1, 0.2, 0.1],])If we do a matrix multiplication between A and AT, the resulting matrix will be the dot-product similarities between all possible pairs of words. For example, the word pool is more similar to badminton than the word beats. In other words, this matrix hints that the word badminton should be more important than the word beats when adding more context to the word embedding of pool.Diagram by AuthorA_At = np.matmul(A, A.T)&gt;&gt;&gt; A_Atarray([[0.31, 0.14, 0.3 ], [0.14, 0.31, 0.15], [0.3 , 0.15, 0.31]])By applying the softmax function across each word, we can ensure that these “similarity scores” add up to 1.0.The last step is to do another matrix multiplication with matrix A. In a way, this step consolidates the contexts of the entire sequence to each embedding in an “intelligent” manner. In the example below, both embeddings of beats and badminton are added to pool but with different weights depending on their similarities with pool.Diagram by Authoroutput = np.round( np.matmul(softmax(A_At, axis=1), A), 2)&gt;&gt;&gt; outputarray([[0.38, 0.22, 0.16, 0.14], [0.35, 0.25, 0.17, 0.13], [0.38, 0.22, 0.17, 0.13]])Notice that the output matrix has the same dimensions (3 x 4) as the original input A. The intuition is that each word vector is now enriched with more information. This is the gist of the self-attention mechanism.Scaled Dot-Product AttentionThe picture below shows the Scaled Dot-Product Attention from the paper. The core operations are the same as the example we explored. Notice that scaling is added before softmax to ensure stable gradients, and there is an optional masking operation. Inputs are also termed as Q, K and V.Image taken from Attention Is All You Need paperThe Scaled Dot-Product Attention can be represented as attention(Q, K, V) function.Diagram by Frank Odom on MediumAdding trainable weights with linear layersThe initial example that we use can be represented as attention(A, A, A), where matrix A contains the word embeddings of pool, beats and badminton. So far there are no weights involved. We can make a simple adjustment to add trainable parameters.Imagine we have (m x m) matrices MQ, MK and MV where m matches the dimension of word embeddings in A. Instead of passing matrix A directly to the function, we can calculate Q = A MQ, K = A MK and V = A MV which will be the same sizes as A. Then we apply attention(Q, K, V) afterwards. In neural network, this is akin to adding a linear layer before each input into the Scaled Dot-Product Attention.To complete the Single-Head Attention mechanism, we just need to add another linear layer after the output from the Scaled Dot-Product Attention. The idea of expanding to the Multi-Head Attention in the paper is relatively simpler to grasp.Diagram by Frank Odom on MediumResources Series on Attention by Rasa Algorithm Whiteboard The Illustrated Transformer by Jay Alammer" }, { "title": "Quick History of JavaScript Module Ecosystem", "url": "/posts/javascript-module-ecosystem/", "categories": "Tech", "tags": "JavaScript", "date": "2022-04-07 03:02:00 +0000", "snippet": "IIFE - Initial Concept of JS ModulesImmediately-invoked Function Expression are anonymous functions that wrap around code blocks to be imported. In the example below, the inner function sayHi() can...", "content": "IIFE - Initial Concept of JS ModulesImmediately-invoked Function Expression are anonymous functions that wrap around code blocks to be imported. In the example below, the inner function sayHi() cannot be accessed outside the anonymous function. The anonymous function itself also does not have a name so it does not pollute the global scope.// script1.js(function () { var userName = \"Steve\"; function sayHi(name) { console.log(\"Hi \" + name); } sayHi(userName);})();If this script is included as shown below, no variable name collision can occur with other scripts such as script2.js.&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;JavaScript Demo&lt;/title&gt; &lt;script src=\"script1.js\"&gt;&lt;/script&gt; &lt;script src=\"script2.js\"&gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;IIFE Demo&lt;/h1&gt; &lt;/body&gt;&lt;/html&gt;Problems with IIFEWhat if script2.js wants to use the sayHi() function defined in script1.js? We can pass a common global variable through the two IIFE modules as shown below.// script1.js(function (window) { function sayHi(name) { console.log(\"Hi \" + name); } window.script1 = { sayHi };})(window);// script2.js(function (window) { function sayHiBye(name) { window.script1.sayHi(name); console.log(\"Bye \" + name); } var userName = \"Jenny\"; sayHiBye(userName);})(window);This solves the immediate problem, but generates other issues.If we reorder script1.js and script2.js, the code will break as the window object will not have the script1 object by the time script2.js starts to load.There is also the problem of what common variable to pass between the two IIFE. One company may use the window object but another may create a new app object in the global scope. No strict standards means incompatiblity issues.CommonJS - Solving the problems of IIFECommonJS is a series of specifications for development of JavaScript applications in non-browser environments. One of the specifications is the API for importing and exporting of modules. This is where require() and module.exports are introduced.There is no more need for passing around a global variable or wrapping an anonymous function around every code blocks for export.// script1.jsfunction sayHi(name) { console.log(\"Hi \" + name);}module.exports.sayHi = sayHi;// script2.jsscript1 = require(\"./script1.js\");function sayHiBye(name) { script1.sayHi(name); console.log(\"Bye \" + name);}var userName = \"Jenny\";sayHiBye(userName);However, CommonJS was not meant for the browser environment. The specifications also do not support asychronous loading of the modules which is important in the browser environment for the user experience.Module Bundler - CommonJS style modules in the BrowserModule bundlers such as Webpack solves the incompatibility problem by bundling CommonJS modules for usage in the browser. The modules are loaded into a single bundle.js file such that individual dependencies are satisfied, which can be loaded onto the page with the a single &lt;script&gt; tag.For the example above, webpack can produce a single bundle.js with script2.js as an entry. The bundle will include script1.js first as it understands the dependency graph. By including the bundle.js into HTML as shown below, the abovementioned problems with CommonJS are fixed.&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;JavaScript Demo&lt;/title&gt; &lt;script src=\"bundle.js\"&gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Webpack Demo&lt;/h1&gt; &lt;/body&gt;&lt;/html&gt;ES6 - Module system as part of JavaScript standardES6 is a JavaScript standard introduced in 2015 that finally introduced a module system for JavaScript in the browsers. ES6 modules utilize import and export keywords. Unlike CommonJS, webpack is not necessary for browser compatibility. We only need to add a type=\"module\" attribute inside the HTML &lt;script&gt; tag and everything will work out of the box.&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;JavaScript Demo&lt;/title&gt; &lt;script type=\"module\" src=\"script2.js\"&gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;ES6 Demo&lt;/h1&gt; &lt;/body&gt;&lt;/html&gt;// script1.jsfunction sayHi(name) { console.log(\"Hi \" + name);}export default { sayHi };// script2.jsimport script1 from './script1.js';function sayHiBye(name) { script1.sayHi(name); console.log(\"Bye \" + name);}var userName = \"Jenny\";sayHiBye(userName);Why are bundlers still used for browser scripts?Backward Compatibility: ES6 modules are not recognized in the older versions of the browsers. Bundlers allow developers to work with the more modern ES6 syntax while the code remains compatible with older browsers.Size Reduction: Minifying code with bundlers reduces file sizes which will lead to faster page loads.Code Splitting: Bundlers can split code into chunks which can then be loaded on demand or in parallel.Caching Support: Webpack can be configured to name the bundles with the hash of their contents. Browsers will only fetch scripts from the server if the hashes no longer match.Resources JavaScript Modules by uidotdev ES6 Modules in the Browser by David Gilbertson JavaScript Module Systems Showdown by Auth0" }, { "title": "Speed up Python applications With Ctypes Library", "url": "/posts/speed-up-python-with-ctypes/", "categories": "Tech", "tags": "Python, C, Concurrency", "date": "2022-04-05 02:00:00 +0000", "snippet": "There are multiple ways to speed up computations in Python. The cython language compiles Python-like syntax into CPython extensions. Libraries such as numpy provides methods to manipulate large arr...", "content": "There are multiple ways to speed up computations in Python. The cython language compiles Python-like syntax into CPython extensions. Libraries such as numpy provides methods to manipulate large arrays and matrices efficiently with underlying C data structures.In this post, I will be discussing the ctypes module. It provides C-compatible data types to so that Python functions can use C-compiled shared libraries. Therefore, we can offload computationally intensive modules of a Python application into C where the developers will have more fine-grained control. To my surprise, this comes as part of the Python standard library, so no external dependencies are required!Code to optimize - prime number checkerI have created a sample program that we can speed up afterwards using the ctypes module. The num_primes() calculates the total number of primes in a list by looping through each item.# prime.pydef is_prime(num: int): for i in range(2, int(num**(0.5))): if num % i == 0: return 1 return 0def num_primes(num_list: List[int]): count = 0 for num in num_list: count += is_prime(num) return countLet’s see the number of primes in a list of 1 million integers. Note that we use consecutive numbers for the example but it does not have to be.from prime import num_primesMAX_NUM = 1000000num_list = list(range(MAX_NUM))def timeit_function(): return num_primes(num_list)print(timeit_function())It takes around 3.4 seconds to run. How can we speed this up?&gt;&gt;&gt; python -m timeit -n 5 -s 'import test_python as t' 't.timeit_function()'Primes: 9212955 loops, best of 5: 3.4 sec per loopWhy Python threading module does not workAs Python has a threading module, one idea is to parallalize calculations across the entire list by using multiple threads. However, this is not possible due to Python’s Global Interpreter Lock (GIL), which prevents multiple threads in a process from executing Python bytecode at the same time. Hence for non-I/O operations, there will not be any speed up.Rewriting prime checker in CThe prime checker is reimplemented in C as shown below, then compiled it into a shared library prime.so. Note that the program logic is exactly the same.// prime.c#include &lt;stdio.h&gt;#include &lt;math.h&gt;int is_prime(int num) { for (int i=2; i&lt;(int)sqrt(num); i++) { if (num % i == 0) return 1; } return 0;} int num_primes(int arrSize, int *numArr) { int count = 0; for (int i=0; i&lt;arrSize; i++) count += is_prime(numArr[i]); return count;}Calling the C-compiled prime checker with ctypesThe ctypes library provides C-compatible data types in Python. All we need to do is load the shared library with CDLL() API and then declare the parameters/ return types accordingly with argtypes and restype attributes.from ctypes import *# Load the shared librarylib = CDLL(\"./libprime.so\")# Declare the return data as 32-bit intlib.num_primes.restype = c_int32# Declare the arguments as a 32-bit int and a pointer for 32-bit int (for list)lib.num_primes.argtypes = [c_int32, POINTER(c_int32)]Afterwards, the num_primes() in the shared library can be called! Note that the num_list has to be converted from Python list into a contiguous array of C with a method provided by ctypes.MAX_NUM = 1000000num_list = list(range(MAX_NUM))def timeit_function(): # num_list is converted into an integer array of size MAX_NUM return lib.num_primes(MAX_NUM, (c_int32 * MAX_NUM)(*num_list))print(f\"Primes: {timeit_function()}\")For the same input of 1 million integers, the speed up is significant just by offloading the same program logic to C code. It makes sense because contiguous arrays in C can leverage caching mechanisms better than lists in Python.&gt;&gt;&gt; python -m timeit -n 5 -s 'import test_ctypes as t' 't.timeit_function()'Primes: 9212955 loops, best of 5: 482 msec per loopMultithreading in the C shared library with POSIX pthreadsThere is one more benefit of offloading the work to C. Since the shared library is not under Python’s GIL, we can now use multithreading in C to parallelize the computations!In the code below, the integer array is split evenly into 4 subarrays and 4 threads are spawned with POSIX pthreads to do parallel work. Each thread runs thread_function() to check the numbers in the array without any overlap between threads. The counts of prime numbers are added into countByThreads array which are summed up after the child threads have terminated.#define NUM_THREADS 4 // 4 threads used// Global variables for spawn threads to accessint *gArrSize = 0; // Ptr for array sizeint *gNumArr = 0; // Ptr for input arrayint countByThreads[NUM_THREADS] = { 0 }; // Prime counts of each threadpthread_t tids[NUM_THREADS] = { 0 }; // IDs of each thread// Function run by each threadvoid *thread_function(void *vargp) { // Each thread has a different offset int offset = (*(int*) vargp); int count = 0; // Split the array items evenly across threads for (int i=offset; i &lt; *gArrSize; i+=NUM_THREADS) count += is_prime(gNumArr[i]); countByThreads[offset] += count; free(vargp);}int num_primes(int arrSize, int *numArr) { gArrSize = &amp;arrSize; gNumArr = numArr; for(int i=0; i &lt; NUM_THREADS; i++) { int *offset = (int*) malloc(sizeof(int)); *offset = i; if(pthread_create(&amp;tids[i], NULL, thread_function, (void *) offset) == -1) exit(1); } int count = 0; for(int i=0; i &lt; NUM_THREADS; i++) { if(pthread_join(tids[i], NULL) == -1) exit(1); // Combine counts from each thread after termination count += countByThreads[i]; countByThreads[i] = 0; } return count;}We have further sped up the code execution although there is an additional overhead of managing threads.&gt;&gt;&gt; python -m timeit -n 5 -s 'import test_ctypes_pthread as t' 't.timeit_function()'Primes: 9212955 loops, best of 5: 322 msec per loopCalling the C-compiled prime checker with Python threadingRemember the threading module in Python just now? Another neat thing about ctypes is that the program releases the GIL as long as the execution is inside the C-compiled shared library. So instead of POSIX pthreads in C, we can generate the threads with threading instead!from ctypes import *# Load the shared librarylib = CDLL(\"./libprime.so\")# Declare the return data as 32-bit integerlib.num_primes.restype = c_int32# Declare the arguments as a 32-bit integer &amp; a pointer for 32-bit integer (for list)lib.num_primes.argtypes = [c_int32, POINTER(c_int32)]Afterwards, the num_primes() in the shared library can be called! Note that the num_list has to be converted from Python list into a contiguous array with a method provided by ctypes.MAX_NUM = 1000000NUM_THREADS = 4# Prime counts per threadcount_list = [0 for _ in range(NUM_THREADS)]# One list of numbers for each threadnum_list_list = []# Split the list for multiple threadsfor i in range(NUM_THREADS): num_list = list(range(i, MAX_NUM, NUM_THREADS)) num_list_list.append(num_list)# Function run by each threaddef thread_function(i, num_list, count_list): len_num_list = len(num_list) count_list[i] = lib.num_primes(len_num_list, (c_int32 * len_num_list)(*num_list))def timeit_function(): threads = [] for i in range(NUM_THREADS): t = threading.Thread(target=thread_function, args=(i, num_list_list[i], count_list)) t.start() threads.append(t) for thread in threads: thread.join() return sum(count_list) # Combine counts from each thread print(f\"Primes: {timeit_function()}\")For this example, the speed up is comparable to using pthreads.&gt;&gt;&gt; python -m timeit -n 5 -s 'import test_ctypes_threading as t' 't.timeit_function()'Primes: 9212955 loops, best of 5: 313 msec per loopThe code demonstrations can be found here.Resources Ctypes Made Easy by Dublin User Group Bypassing GIL with ctypes by Christopher Swenson Python ctypes documentation" }, { "title": "Generators - The Beginning of Asynchronicity in Python", "url": "/posts/async-io-event-loop/", "categories": "Tech", "tags": "Python, Concurrency", "date": "2022-04-03 09:00:00 +0000", "snippet": "If you have worked with asynchronous programming in Python, you may have used the async and await keywords before. It turns out that Python Generators are actually the building blocks of these abst...", "content": "If you have worked with asynchronous programming in Python, you may have used the async and await keywords before. It turns out that Python Generators are actually the building blocks of these abstractions. This article explain their relationship in a greater detail.CoroutinesFor single-threaded asynchronous programming to work in Python, we need a mechanism to “pause” function calls. For example if a particular function involves fetching something from a database, we would like to “pause” the function’s execution and schedule something else until the response is received. However in traditional Python functions, the return keyword frees up the internal state at the end of invocation…It turns out that generators in Python can achieve similar purpose! With generators, the yield keyword gives up the control of the thread while the internal state is saved until the next invocation. So we can do some multitasking with a scheduler as shown below.def gen_one(): print(\"Gen one doing some work\") yield print(\"Gen one doing more work\") yielddef gen_two(): print(\"Gen two doing some work\") yield print(\"Gen two doing more work\") yielddef scheduler(): g1 = gen_one() g2 = gen_two() next(g1) next(g2) next(g1) next(g2)&gt;&gt;&gt; scheduler()Gen one doing some workGen two doing some workGen one doing more workGen two doing more workCoroutine is the term for suspendable functions. As generators cannot take in values like normal functions, new methods are introduced in PEP 342 including .send() that allows passing of parameters (and also .throw() and .close()).def coroutine_one(): print(\"Coroutine one doing some work\") data = (yield) print(f\"Received data: {data}\") print(\"Coroutine one doing more work\") yieldcor1 = coroutine_one()cor1.send(None)cor1.send(\"lorem ipsum\")Coroutine one doing some workReceived data: lorem ipsumCoroutine one doing more workLet’s refer to generators as coroutines from now.Nested coroutinesAnother problem we have is that nested coroutines would not work with current syntax. As shown below, how will coroutine_three() call coroutine_one() and coroutine_two()? It is just a function that has two coroutine objects but has no ability to schedule them!def coroutine_one(): print(\"Coroutine one doing some work\") yield print(\"Coroutine one doing more work\") yielddef coroutine_two(): print(\"Coroutine two doing some work\") yield print(\"Coroutine two doing more work\") yield# Will not work as intendeddef coroutine_three(): coroutine_one() coroutine_two()To solve this, PEP 380 introduces the yield from operator. This allows a section of code containing yield to be factored out and placed in another generator. So in essence the yield calls are “flattened” so that the same scheduler that handles coroutine_three() can handle the nested coroutines. Furthermore, if the inner coroutines use return, the values can made available to coroutine_three(), just like traditional nested functions!def coroutine_three(): yield from coroutine_one() yield from coroutine_two()# Equivalent code# The 'yield' calls in subgenerators are flatteneddef coroutine_three(): print(\"Coroutine one doing some work\") yield print(\"Coroutine one doing more work\") yield print(\"Coroutine two doing some work\") yield print(\"Coroutine two doing more work\") yieldBetter scheduler functionThe previous scheduler in the example interleaves the two coroutines manually. A more automatic implementation will be using a queue as shown below.from collections import dequedef scheduler(coroutines): q = deque(coroutines) while q: try: coroutine = q.popleft() results = coroutine.send(None) q.append(coroutine) except StopIteration: pass&gt;&gt;&gt; scheduler([coroutine_one(), coroutine_two()])Coroutine one doing some workCoroutine two doing some workCoroutine one doing more workCoroutine two doing more workHow coroutines help with asynchronous I/ODuring I/O operations, a synchronous function will block the main thread until the I/O is ready. To carry out asychronous work on a single thread, a good way is for the scheduler to check all the coroutines in the queue and only allow those which are “ready” to run.In the example below, coroutine_four() has to fetch data through I/O operation. While it is suspended as the kernel populates the read buffer, the scheduler allows other coroutines to occupy the thread. The scheduler only allows coroutine_four() to execute again when the I/O is ready.def fetch_data(): print(\"Fetching data awaiting IO..\") # Suspends coroutine while awaiting IO to be ready yield # Let's assume that the scheduler only reschedules # the coroutine again when IO is ready print(\"Fetching data IO ready..\") # Mocked data return 10def coroutine_four(): print(\"Coroutine four doing some work\") data = yield from fetch_data() # I/O related coroutine print(\"Coroutine four doing more work with data: \" + str(data)) yield&gt;&gt;&gt; scheduler([coroutine_one(), coroutine_four()])Coroutine one doing some workCoroutine four doing some workFetching data awaiting IO..Coroutine one doing more workFetching data IO ready..Coroutine four doing more work with data: 10How does the scheduler check for I/O completion?In the previous example, the I/O completion is mocked inside the fetch_data() coroutine. In reality, how does the scheduler know when the I/O is complete?This is where AsyncIO library comes in. It introduces concepts called Future and the Event Loop. Future objects are just coroutines that track whether the results (such as I/O) are ready or not. The Event Loop is just a for-loop that continuously schedules and runs coroutines, similar to our scheduler() function in the examples. At each iteration, the scheduler also polls for I/O operations to see which file descriptors are ready for I/O.In essence, this is the pseudocode of the Event Loop in asyncio.while the event loop is not stopped: poll for I/O and schedule reads/writes that are ready schedule the coroutines set for a 'later' time run the scheduled coroutinesCleaner code with async-awaitEven though coroutines work well with the yield keyword of generators, it was not the original intention of the feature. From Python 3.5 onwards, coroutines were made first-class features with the introduction of async and await keywords.There are some implementation differences but the main features remain the same. For example, assuming that fetch_data() returns an awaitable object, the coroutine_four() can be rewritten as shown below.async def coroutine_four(): print(\"Coroutine four doing some work\") data = await fetch_data() print(\"Coroutine four doing more work with data: \" + str(data))The same coroutine methods such as .send() will still work but the purpose now a lot clearer!Resources AsyncIO Event Loop by EdgeDB AsyncIO by Real Python Yield to Async-await by Mleue Event Loop by Lei Mao" } ]
